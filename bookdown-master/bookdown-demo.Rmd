--- 
title: "Mathematical Demography"
author: "Josh R. Goldstein"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib, references.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This is a book from the Mathematical Demography lecture notes."
---

# Introduction

Mathematical demography book

Special thank you to the Spring 2020 Mathematical Demography students for the problem set answers. 


```{r eval=FALSE, echo=FALSE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

<!--chapter:end:index.Rmd-->

# Introduction to Demographic Heterogeneity {#intro}

## Outline
1. What demographic heterogeneity is (and isn't)
2. Dynamics of population growth with two sub-groups
3. Keyfitz's result $\bar{r}'(t) = \sigma^2_r(t)$.
4. Ken's model of Poisson heterogeneity

## Part I. Conceptual Introduction

### What is Demographic Heterogeneity?
  
- If we see different outcomes (e.g., people dying at different ages), 
is this *Demographic Heterogeneity*? \pause NO.
- Demographic heterogeneity $=$ different rates for different folks.

>In a *demographically heterogeneous* population, people are of
different types, with different type-specific rates.

(These types can be discrete, with individuals being
homogeneous within their type, or they can be continuous with possibly
no individual having exactly the same risk as another.)
  
#### An example
  
  Let's draw 10 individuals from a homogeneous population and
  heterogeneous population.

```{r hetero-homo, fig.cap='Homogeneity and Heterogeneity', out.width='80%', fig.align='center', fig.height=4}
  ## Homogeneous hazard of 1/10
  set.seed(13)
  x.homo <- rexp(10, rate = 1/10)
  ## Heterogeneous hazard (half 1/6 and half 1/13)
  ## Note: I didn't pick these particular numbers for any specific reason
  x.hetero <- c(rexp(5, rate = 1/6),
                rexp(5, rate = 1/13))
  par(mfrow = c(1,2))
  dotchart(x.homo, main = "homogenous variation")
  dotchart(x.hetero, main = "heterogeneous variation")
```

  
- Can you tell which is which?
  - _Homogeneous_: Chance only
  - *Heterogeneous:* Chance + group variation in risk
  
- Would we expect to see a difference if we increased sample?

  - NOTE: ANSWER THIS
  
### Analogies
  - Social inequality: equal opportunity vs. equal outcomes
  - Analysis of variance: $\mbox{total variance} = \mbox{within group} + \mbox{between group}$
  - Statistical models $y = a + b x + \epsilon$
  

### What's new? 
  - Dynamics.
  - Heterogeneous populations evolve differently. 
    - Aggregates $\neq$ Individuals
    - Rates of growth (or decline)
    - Changes over time or age or duration
    - The trajectory of even the \alert{average} individual differs from population average
    - Relative positions, change of groups, may be misleading.
  - Terminology
    - Heterogeneity
    - Unobserved heterogeneity
    - Selection
    - Selective survival
    - Other terms?
  
<!-- \begin{frame}{1. Heterogeneity at work? Black-White Crossover} -->
<!-- \end{frame} -->

<!-- \begin{frame}{1. Heterogeneity at work? Mortality Plateus} -->
<!-- \end{frame} -->


  - Big Caveat: Fundamental Unidentifiability

    - Same data of $N$ observations
      - $N$ draws from 1 distribution
      - $1$ draw from $N$ distributions
      - Something in-between
  

  Abel (66) and Beth (76) example. 

#### A 2nd example: Exponential growth, two countries

  Two countries start equal size, but grow at different rates. What
  happens to aggregate growth rate?

```{r 2-countries, fig.cap='Aggregate growth rate of sub-populations A + B', out.width='80%', fig.align='center'}
  rA = .03 ## growth rate of A
  rB = .01 ## growth rate of A
  KA = 100 ## starting pop size
  KB = 100
  t = 0:200
  KA.t = KA*exp(rA*t) ## exp growth of A
  KB.t = KB*exp(rB*t) ## exp growth of B
  K <- KA.t + KB.t ## combined pop
  r.bar = diff(log(K)) ## growth rate
  plot(t[-1], r.bar, type = "l", ylim = c(0, 0.04),
       ylab = "r.bar", xlab = "time")
  abline(h = c(rA, rB), lty = 2)
```
<!-- title("Aggregate growth rate of sub-populations A + B") -->
- Questions

  - What determines growth rate?
  - How does it change over time?
  - Does the process converge?


#### More examples to work
  1. Differential, constant mortality ($\mu_A = .03$; $\mu_B =.01$)
  2. Differential, _time-varying_ mortality or growth.
  3. ``Movers and Stayers'' (Migration)
  4. ``Movers and Stayers'' (Marriage)
  5. Fecundity: aging or heterogeneity?
  6. Divorce: duration or heterogeneity?
  7. Duration of unemployment: duration or heterogeneity?
  8. Recidivism by time out of prison

### Application

```{r echo=FALSE}
knitr::include_url('https://shiny.demog.berkeley.edu/josh/het_ruse/', height = '600px')
```
- Can you create a plateau?
- Can you create a crossover?
- Can you get aggregate rate to decline?
- Anything else?
  
## Part II. Formal Analysis

### Outline
  1. Keyfitz result
  2. Keyfitz USA-Mexico example
  3. Ken's Poisson-Exponential Model
  
  
### Keyfitz result

  $${d \over dt}\bar{r}(t) = \sigma^2_r(t)$$
  
>When group-specific growth rates are constant the rate of change of the aggregate growth rate equals the variance of the growth rates.


- Derivation

By definition, 
$$ \bar{K}(t) = \sum_i K_i(t) = \sum_i K_i e^{r_i t} $$
and

$$\bar{r}(t) = {{d \over dt} \bar{K}(t) \over \bar{K}(t)}$$

Let's take derivatives and simplify, recalling definition of variance.

SOLVE?

### US-Mexico Example

```{r us-mx, fig.cap='Keyfitz result for US-Mexico', out.width='80%', fig.asp=.75, fig.align='center', fig.height=5}
rm = 3.5/100
ru = .75/100
Km = 50 
Ku = 100
t <- -50:150 ## go back in time to see rise and fall of variance
Kt = Km * exp(t*rm) + Ku * exp(t*ru)
bar.rt <- diff(log(Kt))
par(mfrow = c(2,2))
plot(t, Kt, lwd = 2, type = 'l')
title('Total pop size (solid)\n Group m (dashed)')
lines(t, Km * exp(t*rm), lty = 2, col = "red")
lines(t, Ku * exp(t*ru), lty = 2, col = "blue")
my.v = 26
abline(v = my.v)
plot(t, Kt, lwd = 2, type = 'l', log = 'y', ylim = c(.5, max(Kt)))
lines(t, Km * exp(t*rm), lty = 2, col = "red")
lines(t, Ku * exp(t*ru), lty = 2, col = "blue")
abline(v = my.v)
title("Total pop size (solid)\n Group 'm' (dashed): Log-scale")
plot(t[-1], bar.rt, type = 'l', main = 'Aggregate growth rate')
plot(t[-(1:2)], diff(bar.rt), type = 'l',
main = 'Change in aggregate growth rate')
```

#### Commentary on Keyfitz result

  - Growth rates in heterogeneous populations start at pop average
    and then increase.
  - Heterogeneity pop growth
  - We will extend to cover non-constant growth
  
  But
  
  - Doesn't tell us how much bigger $\bar{K}(t)$ is projection
    using constant aggregate rate $\bar{r}(0)$.
  - Doesn't give us a formula for time path of aggregate $\bar{K}(t)$
    or $\bar{r}(t)$
  
  Note: our homework will try to address some of this using Taylor
  approximation. 
  
### The Origin of Professor Wachter's Poisson-Exponential Model
  
Given a world with many sub-populations, each growing expontentially
at their own rate, what can we say about the time-path of world
population growth?

From an email: 

> Josh asks: Suppose we have a discrete mix of subpopulations
growing at different intrinsic rates r whose maximum is r0. Is
there a handy approximation for the growth path of the aggregate
populations?

>The assumption of a discrete mix is essential here. Otherwise
Tauberian theorems apply and, with a vanishingly small portion of
the population close to the maximum growth rate, we do not obtain
long-run exponential growth.

>I recommend modeling the discrete distribution of growth rates as
a mixture of Poisson distributions.


We are considering

\begin{equation}
  \label{mixture}
\bar{K}(t) = \sum_i e^{r_i t} K_i(0).
\end{equation}

Ken suggests 
\begin{equation}
  \label{ri_def}
r_i = r_0 - s(\lambda) \cdot a,
\end{equation}

* $r_0$ growth rate of the fastest growing sub-population
* $s$ a non-negative Poisson distributed integer
* $\lambda$ the parameter of the Poisson distribution (also
  it's mean and variance)
* $a$ gap between adjacent growth rates.


Example:  sub-populations have growth rates 3, 2, 1, 0, -1, $\ldots$ percent,
then $r_0 = 0.03$ and $a = 0.01$. Sizes of sub-pops determined by
Poisson dis'n


\begin{frame}
\begin{figure}[h]
  \caption{A simulation of heterogeneous growth}
  \centering
  \includegraphics[width = 0.8\textwidth]{../ken_model_figure.pdf}
\end{figure}
\end{frame}


#### Closed-form result

$$
K(t) = K(0) e^{r_0 t} e^{-\lambda (1 - e^{-at})}.
$$

To derive:

- Write out mixture to get 
  $$  K(t) = K(0) e^{r_0 t} \sum_i e^{-sat} f(s)
  $$ 
- Substitute for $f(s)$: $Pois ~ {\lambda^s e^{-\lambda} \over s!}$ 
- Recognize that our mixture contains the series representation of
  $e^{-at}$

Interpretation 
$$
K(t) = K(0) e^{r_0 t} e^{-\lambda (1 - e^{-at})}.
$$

- Dominant term contains the maximum population growth rate $r_0$,
- Second term gives the diminishing effect of the
  sub-populations with smaller population growth rates over time.


 Some further analysis, What is the closed-form expression for $\bar{r}(t)$?


### Some commentary


- Poisson and Exponential ``fit''
- We'll this complementarity again (e.g., with Gamma)
- Tractable  models are super powerful for enhancing our understanding.
- But be careful. Avoid extremes: the model is right/wrong.
  

A BIG caveat, are disaggregated models necessarily better?

- Some potential problems:
  - Aggregate constraints?
  - Interacting sub-populations?
  - Illusion of precision?
  
## Conclusions
  
  - Heterogeneity as variation in risk (not just outcome)
    
  - Constantly growing parts $\neq$ constantly growth whole
    
  - Keyfitz result: Change in growth rate $=$ variance of growth
    rates
    
  - Poisson growth gives us a closed-form solution.

<!--chapter:end:01-heterogeneity.Rmd-->

# Multiplicative Fixed Frailty {#frailty}

## Outline
1. Review of Mortality Mathematics
2. Multiplicative-fixed-frailty and alternatives to it.
3. Population Survival and Hazards under fixed frailty
4. Gamma frailty

Additional resources 

  - @vaupel2014unobserved:
  - @rodriguez2001: detailed hand-out for Princeton class. Many of the same results as @vaupel2014unobserved, but with alternative derivations and notation. Also includes inversion formula and extensions beyond Gamma frailty such as “Inverse Gaussian Frailty”.

### Review of mortality mathematics

  - $\ell(x)$ or $S(x)$ probability of survival to age $x$
  - $\mu(x)$ or $h(x)$ hazard rate at age $x$ ("minus the exponential rate of change in
    survival")

  Let's treat $\mu$ as a definition.
  $$
  \mu(x) \equiv -{d \over dx} \log \ell(x)
  $$

  Can anti-differentiate (integrate) to solve for survival:
  $$
  \ell(x) = s(x) = e^{-\int_0^x \mu(a)\, da}
  $$

### Application: what is $\ell'(x)$?

  -  in words?
  -  taking derivative of $\ell(x)$
  -  interpretation

#### Two special cases
  
  -  Constant hazards $\mu(x) = \mu$. What's $\ell(x)$?
  -  Gompertz hazards $\mu(x) = a e^{b x}$. What's $\ell(x)$?
  

### Extending Keyfitz to mortality

  $$
  {d \over dx} \bar{\mu}(x) = \mbox{average rate of change} - \sigma_\mu^2
  $$

  What is $\bar{\mu}$? It's a weighted average:
  $$
  \bar{\mu}(x) = {\int \mu(x | z) \ell(x | z) p(z) \, dz
    \over
    \int \ell(x | z) p(z) \, dz}
  $$

  To derive Keyfitz extension, differentiate with respect to age
  $x$. (See \vm eq (13)). A good exercise.



Multiplicative Fixed frailty
  For individual $i$,
  $$
  \mu_i(x) = z_i \mu_0(x).
  $$
  
  - $z_i$ "frailty" of the $i$th individual. (Usually thought
    of as a random variable with mean $1$.)
  - $\mu_0(x)$ "Baseline hazard" schedule.  (Also, the
    schedule of a person with $z = 1$).
  
\begin{frame}{What are some alternatives?}

  Let's think of at least three.
  
  ($\beta$, $i$, $\Delta$)
\end{frame}

### fragile
  Which look like multiplicative fixed frailty?
```{r, fig.height = 3, echo = F, message = F, warning = F, hide = T, results = F}
  library(data.table)
  dt <- fread("/hdir/0/fmenares/Book/bookdown-master/data/SWE.Mx_5x5.txt", na.string = ".")
  dt[, x := as.numeric(gsub("-[0-9]*$", "", Age))]
  dt[Age == "110+", x := 110]
  par(mfrow = c(1,3))
  ## proportional
  dt[Year == "1990-1994", plot(x, Male, type = "l", col = "blue", log = "y", ylim = c(10^-4, 1), lty = 1)]
  dt[Year == "1990-1994", lines(x, 2 * Male, type = "l", col = "blue", lty = 3)]
  dt[Year == "1990-1994", lines(x, 0.5 * Male, type = "l", col = "blue", lty = 2)]
  title("Male (5Mx) 1990, x 0.5 and x 1.5")

  ## men and women
  dt[Year == "1990-1994", plot(x, Female, type = "l", col = "pink", log = "y",
                               ylim = c(10^-4, 1))]
  dt[Year == "1990-1994", lines(x, Male, type = "l", col = "blue")]
  title("By Sex (1990-1994)")

  ## time
  dt[Year == "1790-1794", plot(x, Male, type = "l", col = "blue", log = "y", ylim = c(10^-4, 1), lty = 3)]
  dt[Year == "1890-1894", lines(x, Male, type = "l", col = "blue", lty = 2)]
  dt[Year == "1990-1994", lines(x, Male, type = "l", col = "blue")]
  title("Male 1790, 1890, 1990")
```

## Part I. Results from Fixed Frailty


### A simulation

- Our questions
  -  How do we do a micro-simulation, with individuals?
  -  How does fixed frailty fit in?
  -  How do we compute pop survival, hazards, etc.
  -  How does life table of heterogeneous pop differ from baseline?

### Let's derive pop survival (Note: $\bar{s} = \bar{\ell}$) 

Pop survival will be a weighted average of group survival curves

  $$
  \bar{s}(x) = {p(z_1) s_1(x) + p(z_2) s_2(x) + \ldots \over
    p(z_1) + p(z_2)  + \ldots}
  $$
  With continuous $z$ (what are limits of integration?)
  
  $$
  \bar{s}(x) = \int s(x|z) p(z) \, dz
  $$

  Under Multiplicative Fixed Frailty use
  $$ \mu(x|z)  = z \mu_0(x) $$
  
  to derive
  
  $$\bar{s}(x) = \int s_0(x)^z p(z) \,dz.$$


### Now population hazards (stepping stones)

  Definition of hazards:
  
  $$
  \bar{\mu}(x) = - {d \over dx} \log \bar{s}(x)
  $$

  $$
  \bar{\mu}(x) = \mu_0(x) {\int z s_0(x)^z p(z) \, dz \over
    \int  s_0(x)^z p(z) \, dz}
  $$

  $$
  \bar{\mu}(x) = \mu_0(x) \bar{z}(x)
  $$

  Let's fill in steps.


### Rodriguez question
  
Why isn't population hazard a (simple) average of individual hazards?


> Answer: selected survival means that the distribution of frailty at 
age $x$ differs from the starting frailty distribution at age $0$.


The rate of increase in hazards

  (AKA "LAR: Lifetable Aging Rate")
  $$
  \beta(x) = {d \over dx} \log \mu(x)
  $$

  Example:

  What is $\beta(x)$ for Gompertz: $\mu(x) = a e^{bx}$?
  

Vaupel's result

  $$
  \bar{\beta}(x) = \beta_0(x) - \bar{\mu}(x) CV_z^2(x)
  $$

-  Hazards rise less slowly in pop than in baseline
-  If pop hazards plateau, then $\bar{\beta}(x) = 0$
-  Two special cases
  -  Homogeneous pop and plateau in baseline
  -  Gompertz and constant $CV_z$ (e.g., from Gamma)

## Part II. Introduction to Gamma Frailty

### The Gamma distribution
  
-  What do we want in a frailty distribution?
  -  What's the Gamma?
  -  Last math: closed form pop survival
  -  positive?
  -  a single dimension summarizing multiple factors? (Normal?)
  -  flexible?
  -  tractable?


$$
p(z | k, \lambda) = {\lambda^k  \over \Gamma(k)} z^{k-1} e^{-\lambda z} 
$$.

- $z$ the random variable representing frailty
- $k, \lambda$ parameters
- $\Gamma(k)$ A normalizing constant.

### Gamma in R

  - Mean: $k / \lambda$ 
  - Variance: $ k / \lambda^2$
  

```{r}
  ## with k and lambda
  k = 3; lambda = 6
  x <- rgamma(10000, shape = k, rate= lambda)
  mean(x)
  sd(x)
```

Alternate parameterization
```{r}
  ## with mean 1, sigma.sq
  sigma.sq <- .25
  z <- rgamma(10000, shape = 1/sigma.sq, rate = 1/sigma.sq)
  mean(z)
  var(z)
```

### Population Survival of Gamma Frailty

  Big picture
  $$
  \bar{s}(x) = \int s_0(x)^z p(z) \, dz
  $$

  Or, using our definition of survival,
  $$
  \bar{s}(x) = \int e^{-z H_0(x)} p(z) \, dz
  $$


Completing the gamma
  $$
  \bar{s}(x) = \int e^{-z H_0(x)} {\lambda^k  \over \Gamma(k)} z^{k-1} e^{-\lambda z}  \, dz
  $$

 Rearranging,
 
  $$
  \bar{s}(x) = \lambda^k \int  { 1 \over \Gamma(k)} z^{k-1}
  e^{-z (H_0(x)+\lambda)}  \, dz
  $$ 

Integral is like a $Gamma(z | k, H_0(x) + \lambda)$, but missing
something. What?

Our Result

  $$
  \bar{S}(x) = {\lambda^k \over \left[H_0(x) + \lambda\right]^k}
  $$

  If mean = 1.0, then we can let $\lambda = k = 1/\sigma^2$,
  $$
  \bar S(x) = {1/\sigma^2 \over (H_0(x) + 1/\sigma^2)^{1/ \sigma^2}} =
  {1 \over  \left(1 + \sigma^2 H_0(x)\right)^{1/ \sigma^2}}
  $$
  


### Interpreting Gamma-frailty survival
  $$
  \bar S(x) = {1 \over  \left(1 + \sigma^2 H_0(x)\right)^{1/ \sigma^2}}
  $$

  
  -  Older ages, smaller survival.
  -  Variance not so clear, need a picture. (What if $\sigma^2 = 0$?)
  
```{r, fig.height = 3}
x <- 0:100
a = 10^-4
b = 1/10
mx.0 <- a * exp(b*x)
Hx.0 <- cumsum(mx.0)
Sx.0 <- exp(-Hx.0)
## small sigma
sigma.sq = .5^2
bar.S.small.sigma <- 1 / (1 + sigma.sq *Hx.0)^(1/sigma.sq)
## big sigma
sigma.sq = 1^2
bar.S.big.sigma <- 1 / (1 + sigma.sq *Hx.0)^(1/sigma.sq)
plot(x, Sx.0, lty = 2, type = "l", ylim = c(0,1),
       ylab = "Survival")
  lines(x, bar.S.small.sigma, col = "blue")
  lines(x, bar.S.big.sigma, col = "red")
  legend("bottomleft", c("Pop big.sigma",
                       "Pop small.sigma",
                       "Baseline"),
         lty = c(1, 1, 2),
         bty = "n",
         col = c("red", "blue", "black"))
  title("Gamma-frailty population survival")
```

## Conclusions
  - Multiplicative Fixed Frailty is one option for modeling
  - Gave us analytical expressions for population survival and
    hazards including $\bar{\mu}(x) = \mu_0(x) \bar{z}(x)$
  - Extended Keyfitz result to age-changing hazards
  - Survival curve for Gamma
  

<!--chapter:end:02-frailty.Rmd-->



# Gamma Frailty with Applications

  
## From pop survival to pop hazards
  We have
  $$
  \bar S(x) = {1 \over  \left(1 + \sigma^2 H_0(x)\right)^{1/ \sigma^2}}
  $$

  Let's compute $\bar\mu(x)$.

  
  $$
  \bar\mu(x) = {\mu_0 \over 1 + \sigma^2 H_0(x)}
  $$

### What happens to frailty of survivors?

  Recall that pop hazards = baseline $\times  \bar{z}(x)$.

  So,
  $$
  \bar\mu(x) = \mu_0(x) { 1  \over 1 + \sigma^2 H_0(x)}
  $$
  Sketch $\bar{z}(x)$. Hint: what form does $H_0(x)$ have?

#### Example: Gamma-Gompertz
  
  \item If $H_0(x)$ be Gompertz, we have closed-form
    expression. What is it?
  \item Does $\bar{z}$ have the form
    $$
    {1 \over 1 + v*e^{w x}}
    $$
  \item This is a backwards S, going down.
  
```{r, fig.height = 3}
sigma.sq = .2
x = 0:100
a = 5 * 10^-4
b = 1/8
H0.x = (a/b) * (exp(b*x) - 1)
bar.z = 1 / (1 + sigma.sq * H0.x)
plot(x, bar.z)
```

Look at the apparent exponential decline in tail
Homework: what is proportional rate of change in $\bar{z}$ as $x$ 
gets big? Is it close to Gompertz $b$?

Average frailty in terms of survival

  $$ 
  \bar{z}(x) = [\bar{S}(x)]^{\sigma^2}!
  $$

  In real life, we observe $\bar{S}(x)$. So this allows us to say
  something about implied $\bar{z}$ from hazards.

  
  Reversing the logic: if we see a characteristic changing with age,
  then we can estimate "${\sigma^2}$" (I put in quotes because its
  the variance of the proportional effect of the observed
  characteristic.)

## Selection and observed frailty in CenSoc

We have a large matched sample from the 1940 census to Social Security death data observed from 1975 to 2004.  This means that we can compute the survival curves of extinct cohorts and see how mortality selection changes the composition of the cohort as it ages.

In this example, we use observed wage income in 1940 for the cohort born 1895 to 1900. We look at how wages of survivors increase with age as a result of selective mortality and we see if the gamma-frailty model can produce similar results.  

### Data

Read in the data and transform the variables to what we want. We produce a variable $y$ (in this case a standardized version of log wage income) to be transformed into a frailty score.


```{r echo=TRUE}
## read in dat
library(data.table)
dt <- fread("/data/josh/CenSoc/censoc_bfdw.csv")
## Clean wage data
dt[, incwage := INCWAGE]
dt[incwage == 999998, incwage := NA]
dt[incwage == 0, incwage := NA]
```

```{r message=FALSE}
hist_incwage <- dt[, hist(incwage)]
hist_ln_incwage <- dt[, hist(log(incwage))]
```

```{r}
## Do age at death for 1895-1900 cohorts
dt[, age.at.death := dyear + dmonth/12 - (byear + bmonth/12)]
my.dt <- dt[byear %in% 1895:1900 & dyear %in% 1975:2004]
## now limits to deaths younger than 105
my.dt[, max(age.at.death), by = byear]
nrow(my.dt[age.at.death >= 105])
nrow(my.dt[floor(age.at.death) == 104])
## now we have same age range for every cohort
my.dt <- my.dt[age.at.death < 105]
my.dt <- my.dt[!is.na(incwage)] ## keep only non-missing 
```
Log-wages look reasonable, unimodal, kind of symmetric. We now center our variable to 0 before estimating the effect on mortality. Our model exponentiates this 0 to become 1, which is where we want our frailty measure to be centered.

```{r}
## standardized log income
## log_inc_stan = log(y_orig) - mean(log(y_orig))
## note: control for byear, since different ages in 1940
my.dt[, y_orig := incwage]
my.dt[, log_inc := log(incwage)]
my.dt[, log_inc_mean := mean(log_inc), by = byear]
my.dt[, y := log_inc - log_inc_mean]
hist_y <- my.dt[, hist(y)]
my.dt[, summary(y)]
```
Now we're centered at 0.


Show how our (life-long fixed) characteristic of interest changes by age because of mortality selection.

```{r include=FALSE}
x <- 74:104
y.bar <- NULL
y_orig.bar <- NULL
## z.bar <- NULL
for (i in 1:length(x))
{
    y.bar[i] <- my.dt[age.at.death > x[i], mean(y)]
    y_orig.bar[i] <- my.dt[age.at.death > x[i], mean(y_orig)]
##    z.bar[i] <- my.dt[age.at.death > x[i], mean(z)]
}
```

```{r}
par(mfrow = c(1,2))
my.dt[, plot(x, y_orig.bar)]
title("Wage income by surviving age", cex.main = .7)
my.dt[, plot(x, y.bar)]
title("Standardized log of Wage income by surviving age", cex.main = .7)
```

So we see annual wage income in 1940 increases by about $100 or so, or about 5% from age 75 to age 95. And more after that.

> Is this what we would expect from our Gamma frailty model?

### Estimation

Estimate an observed frailty for each person, call this $z_{obs}$ To do this we first use  Cox regression to estimate the proportional effect of $y$ on hazards. The Cox model has the form


$$
\mu_i(x) = \mu_0(x) e^{\beta  y}
$$

We can then transform $y$ into a frailty score $z_{obs}$, letting
$$
z_{obs} = e^{\hat\beta  y}
$$

```{r}
## now get z's
library(survival)
my.dt[, event := 1]
m <- coxph(Surv(age.at.death, event) ~ y, data = my.dt)
(summary(m))
##                      coef exp(coef) se(coef)      z Pr(>|z|)
## y -0.03073   0.96974  0.00177 -17.36   <2e-16 ***
## so a 10% increase in income reduces mortality by .3% (quite a tiny effect!)
beta <- coef(m)

```

> The effect is very small. Any ideas why?

Now estimate our z's.

```{r}
my.dt[, z := exp(beta * y)]
hist_z <- my.dt[, hist(z, xlim = c(0, 3))]
```

> Does this look gamma-like?


Calculating the variance of $z_{obs}$ to be used for estimating $\bar{z}_{obs}$. (Also plotting the histogram to see if it looks gamma-like)


```{r}
sigma.sq <- var(my.dt$z)
print(sigma.sq)
print(sqrt(sigma.sq))
```
> Check SD against histogram. Does it look right?


Extinct cohort method to estimate survivorship $\bar{S}(x)$

```{r}
Dx <- my.dt[, table(floor(age.at.death))]
par(mfrow = c(1,2))
plot(Dx)
lx <- rev(cumsum(rev(Dx)))
lxpn <- c(lx[-1],0)
Lx <- (lx + lxpn)/2
mx <- Dx/Lx
x <- as.numeric(names(Dx))
plot(x, log(mx), type = "p")
## gomp fit from ages 80 to 95
m <- lm (log(mx) ~ x, subset = x %in% 80:100)
lines(80:100, predict(m), lwd = 2)
axis(2)
## no slowdown in mortality!!
```
> How does Dx look? Plausible?

> How about hazards? They are Gompertzian for a while, but how do we explain tails?

```{r}
plot(x, lx)
```



Estimation of $\hat{\bar{z}}(x)$ using the gamma-frailty result:

$$
\bar{z}(x) = \bar{S}(x)^{\sigma^2}
$$


```{r}
sx.bar <- lx/lx[1]
z.bar.hat <- sx.bar^sigma.sq
```

Comparing this to our observed $\bar{z}$

```{r}
x <- 74:104
z.bar <- NULL
for (i in 1:length(x))
{
    z.bar[i] <- my.dt[age.at.death > x[i], mean(z)]
}
```

Plotting comparison

```{r}
plot(x, z.bar.hat, ylim = c(.995, 1.001), type = "l", lty = 2)
lines(x, z.bar)
```

> How did we do?

> I'm not quite sure why intercept of observed is not exactly 1.0. (Feel free to play around, but I don't think this is important -- I hope.)

Showing plots for observed mortality selection and the gamma-frailty based estimate of mortality selection. Do this for several measures including  $y$, and raw (unstandardized) income.

```{r}
y.bar.hat <- log(z.bar.hat)/beta
plot(x, y.bar, ylab = "log(inc) - mean(log(inc))")
lines(x, y.bar.hat)
title("Mean standardized log-income")
legend("topleft", legend = c("observed", "fitted"),
       pch = c(1, -1), lty = c(-1, 1))
##
( bar.log.y = mean(log(my.dt$y_orig)) )
y_orig.bar.hat.wrong <- exp(y.bar.hat) * exp( bar.log.y) ## this is geometric mean
y_orig.bar.hat.right <- exp(y.bar.hat)*y_orig.bar[1]
plot(x, y_orig.bar, ylab = "$ per year")
title("Mean income")
legend("topleft", legend = c("observed", "fitted"),
       pch = c(1, -1), lty = c(-1, 1))
lines(x, y_orig.bar.hat.wrong, lty = 2)
lines(x, y_orig.bar.hat.right, lty = 1)
```

###  Discuss our conclusions and possible future directions to follow.

> How did we do? Does our gamma frailty model give basically the right prediction?

> How come it appears that wage income matters so little?

> How could we improve the measurement of wage income?

> What other variables could we look at? 

> How would we expect the gamma model do with another variable, e.g. educational attainment?

> What is the relationship between "observed" and "unobserved" frailty?

> IMPORTANT: Is our work here a validation of the model's applicability to real life? If so what are we validating?  That our transformed covariate is roughly gamma distributed? Are we assuming multiplicative fixed frailty -- or are we validating it's applicability?




<!--chapter:end:03-gamma_censoc_application.Rmd-->

# Convergence and cross-overs

## Outline
- Concepts
- Student Presentation

Additional resources:   

  - @coale1986mortality: Non-technical overview of data issues that could create the appearance of mor-tality cross-overs.  
  - @manton1981methods: interesting discussion on parameter choice of the Gamma and the empirical results.  


## What happens to mortality disparities at older ages?
  
  - Cumulative disadvantage
  - Age as a leveler
    - Individual adaptation/plasticity, gov support, separation from
    unequal structures like labor market
  - Bad data / measurement
    - Unreliable ages, institutionalization changes sample, etc.

  - Nothing
    - It's all selection ("frailty"), pop hazards but
    individual hazards would have remained "parallel".

Our goal is to examine this last "null hypothesis". What can  
  frailty explain, and what can't it?
  
### A possible null-model

- 2 groups, each with internal gamma-frailty
- proportional baseline hazards

  $$
  \mu_2(x) =  R \mu_1(x)
  $$
see V&M (38)

  $$
  \mu_1(x |z_1) = \mu_1 z_1 \\ \mu_2(x |z_2) = \mu_2 z_2  
  $$

  And, frailty terms are each gamma, with mean 1 and own variances.

### A result: \vr (5E)

  $$
  \bar{R}(x) \equiv {\bar\mu_2(x) \over \bar\mu_1(x)} =
  {R + R\sigma_1^2 H_1(x) \over 1 + R \sigma_2^2 H_1(x)}
  $$
  
  Questions:
  
  \item If variances are equal. What happens at age 0? What happens
    at very old ages.
  \item If the higher mortality group has bigger frailty variance, what
    happens at older ages?
  \item Same if  higher mortality group has smaller frailty
    variance?
  

> Homework: prove this, simulate this. see if cross of is when
  cumulative hazards satisfy the condition at the end of 5E). (*
  problem. can you solve for x0 in temrs of variances 1 and 2 and R
with gamma gompertz?

### Inversion

Our challenge is to invert a not easy pop hazards formula
$$
\bar{\mu}(x) = {\mu_0(x) \over 1 + \sigma^2 H_0(x)}
$$
because we have both hazards and cumulative hazards on right.

Hazards are slope of log survival

Recall for Gamma,
$$
\bar{S}(x) = { 1 \over (1 + \sigma^2 H_0(x))^{1/\sigma^2}}
$$

We write down the hazard as the derivative of log survival
$$
\bar{\mu}(x)  = {1 \over \sigma^2} {d \over dx} \log(1 + \sigma^2 H_0(x)).
$$

The anti-derivative of both sides, gives
$$
\bar{H}(x) = {1 \over \sigma^2} \log(1 + \sigma^2 H_0(x)).
$$

And now we have only 1 expression involving the baseline hazards on
the right. 


Solving 
$$
\bar{H}(x) = {1 \over \sigma^2} \log(1 + \sigma^2 H_0(x)).
$$
gives us the cumulative hazard
$$
H_0(x) = {1 \over \sigma^2} \left(e^{\sigma^2 \bar{H}(x)} - 1 \right).
$$

And differencing, gives us a remarkably simple expression for the
baseline hazard in terms of the observed popualtion hazard
$$
\mu_0(x) = \bar\mu(x) e^{\sigma_2 \bar{H}(x)}
$$

- We don't observe underlying baseline hazard $\mu_0$ on left
- What is observed (and unobserved) on right?


### An example

```{r} 
library(data.table)
dt <- fread("/hdir/0/fmenares/Book/bookdown-master/data/ITA.cMx_1x1.txt",
            na.string = ".")
my.dt <- dt[Year == 1915]
my.dt[, H.f := cumsum(Female)]
my.dt[, H.m := cumsum(Male)]
my.dt[, h.f := Female]
my.dt[, h.m := Male]
sigma.sq <- .5^2
my.dt[, h0.f.5 := h.f * exp(sigma.sq *H.f)]
my.dt[, h0.m.5 := h.m * exp(sigma.sq *H.m)]
sigma.sq <- .2^2
my.dt[, h0.f.2 := h.f * exp(sigma.sq *H.f)]
my.dt[, h0.m.2 := h.m * exp(sigma.sq *H.m)]
sigma.sq <- 1^2
my.dt[, h0.f1 := h.f * exp(sigma.sq *H.f)]
my.dt[, h0.m1 := h.m * exp(sigma.sq *H.m)]
```


Italian Females, born 1915 ($\sigma^2 = .2^2, .5^2, 1^2$)

```{r fig.height=6, warning=FALSE}
par(mfrow = c(1,2))
foo <- my.dt[, plot(Age, H.f, col = "red")]
title("Cumulative Hazards\n Italian Females born 1915")
foo <- my.dt[, plot(Age, log(h.f), type = "l", ylim = c(-7, 2), col = "red",
                    main = "Observed vs. implied baseline")]
foo <- my.dt[, lines(Age, log(h0.f.2), lty = 2, col = "red")]
foo <- my.dt[, lines(Age, log(h0.f.5), lty = 3, col = "red")]
foo <- my.dt[, lines(Age, log(h0.f1), lty = 4, col = "red")]
legend("topleft", legend = c("baseline", "obs if s2 = .2^2", "obs if s2 = .5^2", "obs if s2 = 1^2"),
       col = "red", lty = 1:4)
```



Italian Females vs Males born 1915 ($\sigma^2 = .5^2$)
```{r fig.height=6, warning=FALSE}
par(mfrow = c(1,2))
## title("Cumulative Hazards\n Italian Females born 1915")
foo <- my.dt[, plot(Age, log(h.f), type = "l", ylim = c(-7, 2), col = "red",
                    main = "Observed vs. implied baseline")]
foo <- my.dt[, lines(Age, log(h0.f.5), lty = 2, col = "red")]
ugh <- my.dt[, lines(Age, log(h.m), type = "l", col = "blue")]
ugh <- my.dt[, lines(Age, log(h0.m.5), lty = 2, col = "blue")]
legend("topleft", legend = c("female observed",
                             "female baseline",
                             "male observed",
                             "male baseline"),
       col = c("red","red", "blue", "blue"), lty = c(1,2, 1,2))
## now do difference
foo <- my.dt[, plot(Age, h.m/h.f, type = "l", col = "black",
                    ylab = c("h.m/h.f"),
                    main = "Male-female hazard ratio")]
foo <- my.dt[, lines(Age, h0.m.5/h0.f.5, lty = 2)]
legend("topright", legend = c("observed",
                              "implied baseline"),
##       col = c("red","blue"),
       lty = 1:2)
```

Much bigger convergence in ``observed'' than in baseline

### Application

```{r echo=FALSE}
knitr::include_url('https://grodri.shinyapps.io/heterogeneity/', height = '600px')
```

## Student Presentation
  

<!--chapter:end:04-crossover.Rmd-->

# Mortality plateaus

## Outline
- Math
- Ken's class
- Presentation


## Heterogeneity slows mortality improvement 

  Define $\rho(x,t)$ be the rate of mortality \alert{improvement}
  $$
  \rho(x,t) = - {d \over dt} \log \bar\mu(x,t)
  $$
  
  
  Extending our gamma result for 1 cohort to the surface, 
  $$
  \bar\mu(x,t) = \mu_0(x,t) \bar{S}_c(x,t)^{\sigma^2}
  $$ 

  We take the log and the time-derivative of hazards  give \vm (39^*)
  $$ 
  \rho(x,t) = \rho_0(x,t) - \sigma^2 {d \over dt} \log
  \bar{S}_c(x,t)^{\sigma^2}
  $$ 

  So individual risks from one cohort to the next are going down
  faster it seems. Intuition?
  
### An example

  Assume  $\sigma^2 = .2$.

```{r}
library(data.table)
  sigma.sq = .2
  dt <- fread("/hdir/0/fmenares/Book/bookdown-master/data/ITA.bltcoh_1x1.txt", na.string = ".")
  mx.80.c1880 <- dt[Year == 1880 & Age == "80"]$mx
  mx.80.c1900 <- dt[Year == 1900 & Age == "80"]$mx
  (rho.bar.80 <- -log(mx.80.c1900/mx.80.c1880)/20) ## about 0.8%
  Sx.80.c1880 <- dt[Year == 1880 & Age == "80"]$lx
  Sx.80.c1900 <- dt[Year == 1900 & Age == "80"]$lx
  (d.log.Sx <- log(Sx.80.c1900/Sx.80.c1880)/20)
  (rho.0.80 = rho.bar.80 + sigma.sq * d.log.Sx) ##  about 1.3%
```
So mortality progress is more than 50\% faster than it appears!

Issues?

## Conclusions 
- Gamma frailty gives simple expressions for population survival,
hazard, and average frailty.
- Gamma frailty gives a plateau
- Gamma frailty gives us a predicted rate of convergence and
cross-over with age
- All of this means it is a useful null model.
- Takes us away from “it could be selection” to “what if it were
selection”


## Ken's class - Hazards and Plateaus
Outline

1. Extremes of Longevity in Humans and Other Species 
2. Gamma-Gompertz Fixed Frailty Hazards 
3. Less Restrictive Frailty Models
4. Mutation Accumulation, Gompertz Hazards with Plateaus

Note: we thank Professor Kenneth Wachter for letting us use the presentation and code from his class on Hazards and Plateaus on February 20, 2020 at Demography UC Berkeley. 

### Extremes of Longevity in Humans and Other Species 
```{r plateaubarbi, fig.cap='Estimated plateau for cohort of Italian women from 1904. Source: @barbi2018', out.width='80%',fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/plateau_1.png')
    ```

   What is the importance of plateaus?
   
   - They encourage optimism that future progress against old-age mortality is feasible. Our bodies are not facing an endlessly mounting set of things going wrong.
   - They point to commonalities with the life-course demography of other species. The shared genetic heritage of advanced organisms is permissive.
   
   For instance, we can compare human hazard to that of other species:  
```{r plateauhoriuchi, fig.cap='Hazards across selected species. Source: @horiuchi2003', out.width='80%', fig.align='center', fig.height=4,  echo=FALSE}
    knitr::include_graphics('figures/plateau_2.png')
    ```   
    
### Gamma-Gompertz Fixed Frailty Hazards
- The Gamma-Gompertz model of @vaupel1979 is familiar from this course and Chapter 8 of Essential Demographic Methods [@wachter2014essential].
- Gamma Gompertz models are one way of generating plateaus for population hazards out of increasing individual hazards.
- We fit parameters to the data in @barbi2018 and look for a predicted plateau, starting from age 60 onwards.
- We need formulas for the individual hazard $h_x$ , for the individual
cumulative hazard $H_x$, and for the aggregate population hazard $\mu_x$ implied by Gamma-distributed frailty with shape parameter $k$ and rate parameter $\theta = k$.
- Does the prediction show a plateau?
- Does the level of the plateau fit plateau observed in the Italian cohorts over age 105?   

  To answer these questions, we create the process below to calculate the aggregate population hazard function predicted from Gompertz hazards for individuals combined with a Gamma distribution for fixed proportional frailty, using parameters matched to the Italian cohort data for @barbi2018.  
```{r, echo=TRUE}
    # Parameters for Gompertz and Gamma 
    beta     <-  0.088            #  Gompertz slope parameter,  Barbi et al.
    alpha60  <-  0.01340671       #  Gompertz intercept parameter at age 60 
    asymp    <-  0.620            #  Observed asymptote, for comparisons in plots
    khat     <-  7.045455         #  Gamma shape parameter, to fit plateau 
    theta    <-  1/khat           #  Gamma rate parameter, for unit mean at 60
    # Age range, starting to observe cohorts at age 60, reaching up to 130 
    xxx      <-  c(0:200)          #  set of values of x, years past 60
    age      <-  60  +  xxx       #  age  

    # Hazards: 
    ###  Gompertz formula to calculate individual hazards hx at x:
    hx       <-  alpha60 *exp(beta*xxx) ##also noted as mu_0(x) in course material
    ###  Individual cumulative hazard  Hx formula
    Hx       <-  cumsum(hx) ## another way of getting it is  (a/b)(exp(bx)-1)
    ###  Aggregate population hazard mu_x with Gamma frailty:
    mux       <-  hx *khat/ (khat + Hx)
```
```{r,  fig.cap='Hazards and plateau with Gamma frailty', out.width='80%', fig.align='center', echo=FALSE }
    #Plot:
    plot(age, log(hx), type= "l", col="blue", xlim = c(60,130), ylim =c(min(log(hx)),4),
         xlab= "Ages for estimated Gompertz", ylab="Log hazard") #individual hazard
    lines(age, log(mux), col="red")           #predicted population hazard
    segments(x0 = 104, x1 =117, y0 = log(asymp) ) #estimated plateau from Barbi et al.
    legend("topleft", legend=c(expression("Individual hazard, h"[x]), 
                               "Predicted population hazard, "~ mu [x],
                               "Plateau estimate Barbi et al. (2018)"),
       col=c( "blue","red", "black"), lty=rep(1,3))
```
  Note that the x-axis shows the ages for the estimated Gompertz, that is for a population of ages 60 and above. $\mu_x$ appears to reach its own plateau due to the chosen parameters from the Gamma-Gompertz model. In this case, we get a plateau further along than in the italian case of @barbi2018. Gamma gompertz models are good at fitting plateaus mathematically but may not be aplicable with the data.  
 
#### How does Gamma Gompertz Make a Plateau?  
- Try alternative values for $k$ and for $\beta$. Guess a formula for the level of the plateau.  
```{r, echo=FALSE, warning=FALSE}
beta_vals <- c(0.001, 0.1, beta, 0.5,1,10)
k_vals <- c(0.1, 1, 3, khat,10)

plateau_beta_nolegend <- function(beta_vals, k_vals){
hx       <-  alpha60 *exp(beta_vals*xxx) ##also noted as mu_0(x) in course material
Hx       <-  cumsum(hx) ## another way of getting it is  (a/b)(exp(bx)-1)
mux      <-  hx *k_vals/ (k_vals + Hx)
    
    #Plot:
    plot(age, log(hx), type= "l", col="blue", xlim = c(60,130), ylim =c(min(log(hx)),4),
         xlab= "Ages for estimated Gompertz", ylab="Log hazard",
         main=bquote(beta == .(beta_vals))) #individual hazard
    lines(age, log(mux), col="red")           #predicted population hazard
    segments(x0 = 104, x1 =117, y0 = log(asymp) ) #estimated plateau from Barbi et al.
}
plateau_beta_legend <- function(beta_vals, k_vals){
hx       <-  alpha60 *exp(beta_vals*xxx) ##also noted as mu_0(x) in course material
Hx       <-  cumsum(hx) ## another way of getting it is  (a/b)(exp(bx)-1)
mux      <-  hx *k_vals/ (k_vals + Hx)
    
    #Plot:
    plot(age, log(hx), type= "l", col="blue", xlim = c(60,130), ylim =c(min(log(hx)),4),
         xlab= "Ages for estimated Gompertz", ylab="Log hazard",
         main=bquote(beta == .(beta_vals))) #individual hazard
    lines(age, log(mux), col="red")           #predicted population hazard
    segments(x0 = 104, x1 =117, y0 = log(asymp) ) #estimated plateau from Barbi et al.
           legend("bottomright", legend=c(expression("Individual hazard, h"[x]), 
                               "Predicted population hazard, "~ mu [x],
                               "Plateau estimate Barbi et al. (2018)"),
                  col=c( "blue","red", "black"), lty=rep(1,3), cex=0.7) 

}


plateau_k_nolegend <- function(beta_vals, k_vals){
hx       <-  alpha60 *exp(beta_vals*xxx) ##also noted as mu_0(x) in course material
Hx       <-  cumsum(hx) ## another way of getting it is  (a/b)(exp(bx)-1)
mux      <-  hx *k_vals/ (k_vals + Hx)
    
    #Plot:
    plot(age, log(hx), type= "l", col="blue", xlim = c(60,130), ylim =c(min(log(hx)),4),
         xlab= "Ages for estimated Gompertz", ylab="Log hazard",
         main=bquote(k == .(k_vals))) #individual hazard
    lines(age, log(mux), col="red")           #predicted population hazard
    segments(x0 = 104, x1 =117, y0 = log(asymp) ) #estimated plateau from Barbi et al.
}
plateau_k_legend <- function(beta_vals, k_vals){
hx       <-  alpha60 *exp(beta_vals*xxx) ##also noted as mu_0(x) in course material
Hx       <-  cumsum(hx) ## another way of getting it is  (a/b)(exp(bx)-1)
mux      <-  hx *k_vals/ (k_vals + Hx)
    
    #Plot:
    plot(age, log(hx), type= "l", col="blue", xlim = c(60,130), ylim =c(min(log(hx)),4),
         xlab= "Ages for estimated Gompertz", ylab="Log hazard",
         main=bquote(k == .(k_vals))) #individual hazard
    lines(age, log(mux), col="red")           #predicted population hazard
    segments(x0 = 104, x1 =117, y0 = log(asymp) ) #estimated plateau from Barbi et al.
           legend("topleft", legend=c(expression("Individual hazard, h"[x]), 
                               "Predicted population hazard, "~ mu [x],
                               "Plateau estimate Barbi et al. (2018)"),
                  col=c( "blue","red", "black"), lty=rep(1,3), cex=0.7) 

}
```
  - Here, we change the values of $\beta$ while fixing the $k$ shape factor of the Gamma frailty distribution. As $\beta$ increases, the individual and population hazards rise (or shift upwards). For comparison, the third panel contains the $\beta=0.088$  and $k=7.045455$ values used in the previous excercise.
```{r,warning=FALSE,fig.cap='Changing values of beta in Gompertz hazards', fig.align='center',  echo=FALSE, message=FALSE}
par(mfrow=c(2,3))
plateau_beta_nolegend(beta_vals[1],khat)
plateau_beta_nolegend(beta_vals[2],khat)
plateau_beta_nolegend(beta_vals[3],khat)
plateau_beta_nolegend(beta_vals[4],khat)
plateau_beta_legend(beta_vals[5],khat)
```
  - The following graphs fix $\beta=0.088$ but use different values of $k$. Contrary to what happens when $\beta$ varies, when the shape parameter of the Gamma frailty distribution changes, only the population hazards appear to change. Population hazards expand upwards with increases in $k$. The fourth panel uses the baseline  values for $\beta=0.088$ and $k=7.045455$.
```{r,warning=FALSE,fig.cap='Changing values of k (Gamma shape parameter)', fig.align='center', echo=FALSE}
par(mfrow=c(2,3))
plateau_k_nolegend(beta,k_vals[1] )
plateau_k_nolegend(beta,k_vals[2] )
plateau_k_nolegend(beta,k_vals[3] )
plateau_k_nolegend(beta,k_vals[4] )
plateau_k_legend(beta,k_vals[5] )
```
- This formula can be proved by showing that $h_{x}/H_{x}$ goes to $\beta$ in a Gompertz model and plugging into the formula for Gamma Gompertz aggregate hazards. 
(**Note for Josh: I don't think Ken really explained this in class and I'm confused on how to answer this.**)
- What happens to the mean frailty of survivors as $x$ increases? What has to balance what in order for a plateau to appear?  
  - First, the mean frailty among survivors is the ratio of  $\mu_x$ to $h_x$. In other words, the mean frailty among the people that remain is the population hazard divided by the individual hazard. 
  - The mean frailty falls across ages, such that around 105 years, the mean frailty is about 0.5. It continues decreasing until it reaches a plateau of about 0.1 starting from age 130 onwards. Survivors past 130 have a low mean frailty but at a higher hazard, as seen by the plateau from previous graph. 
  - The plateau comes from the people that are low frailty but that die. This loss balances out the increase that everyone is experiencing. Selection is balancing senescence. 

```{r,warning=FALSE,fig.cap='Mean frailty', fig.align='center', echo=FALSE}
plot( age, mux/hx, xlim = c(60, 200), type = "l"  , xlab = "", ylab = "Mean Frailty" )
abline( h = 0.5,  lty = 3     )                     # vertical age 105   
abline( h = 0.1,  lty = 3     )                     # vertical age 105   
```


#### Troubles with Gamma Gompertz:  
- Poor fits to observed cohort plateaus.  
- Unrealistic mortality rates at younger ages for those who do survive out onto the plateau. (The frailty distribution reaches down to zero frailty).  
- Individual centenarians still experience exponentially increasing hazards under the model.  
- Why should frailty remain fixed across life?  
- The model assumes rather than explains an underlying Gompertz.  
- There is no full genetic or evolutionary story. 

#### Mathematics of Frailty as a Gamma Random Variable:    
- Probability density:  $\left(\frac{\theta^{k}}{\Gamma(k)}\right)z^{k-1}e^{-\theta z}$  
- Mean: $\frac{k}{\theta}$  
- Variance: $\frac{k}{\theta^2}$  
- $\mathbb{E}e^{-ZH}= \left(\frac{\theta}{\theta  H}\right)^{k}$  
- Exponential and Gamma Random Variables:  
    - When U has a uniform distribution on $[0, 1]$, then $Y = −log(U)/\theta$ has an exponential probability distribution on $(0,\infty)$ with mean $1/\theta$ and variance $1/\theta^2$.  
    - The sum of $k$ independent exponential random variables with the same mean $1/\theta$ has a gamma probability distribution with shape $k$, scale $\theta$, rate $1/\theta$, mean $k/\theta$ and variance $k/(\theta^2)$.  
    - With an exponential variable Y, we have $\mathbb{E}e^{-YH}=\int e^{-yH}p(y)dy= \frac{\theta}{\theta+H}$.
    - Because expectation values of independent random variables multiply, with a gamma variable $Z = Y_1 + Y_2 + · · ·Y_k$, we have  $\mathbb{E}e^{-ZH}= \left(\frac{\theta}{\theta+H}\right)^k$.

#### @vaupel1979  
- Lifelong Fixed Frailty Z.  
- The Gamma distribution has $\mathbb{E}e^{-ZH}= \left(\frac{\theta}{\theta+H}\right)^k$  
- For starting mean frailty equal to 1, we have $k = \theta$.   
- The aggregate population hazard is minus the slope of the logarithm of survivorship:
$\mu_x = -\frac{d}{dx}\left(k  log(\theta)- k  log(\theta+H_x)\right)= \left(\frac{k}{\theta+H_x}\right)h_x$  
- For the Gamma Gompertz, we insert Gompertz individual hazards $h_x$ and individual cumulative hazards $H_x$ into the formula.  
    
### Less Restrictive Frailty Models  
- The key property of the Gamma Gompertz model is a frailty distribution that extends all the way down to zero frailty.  
- With any initial frailty distribution that extends down to zero and looks like a Gamma near zero, the frailty distribution among survivors comes to look like a Gamma.  
- If you do assume a Gamma distribution for frailty, then a plateau in the aggregate population hazard function entails an individual hazard function tending toward a Gompertz.  
- However:  
    - Frailties near zero are unrealistic.  
    - With frailties bounded away from zero, plateaus in aggregate hazards require plateaus in individual hazards  
    - Plateaus in individual hazards can arise naturally, for instance from genetic models.  
- Consider a “semi-circle” frailty whose probability density function is, for instance, $p(z) = (6)(z − 0.7)(1.3 − z)$ on $[0.7, 1.3]$, zero else.  
- If the individual hazard were Gompertz, would there be a plateau?  
- Suppose the individual hazard were given by $\frac{x}{(1 + x)}$.  
    - Would there be a plateau in the individual hazard? If so, at what level?  
    - Would there be a plateau in the aggregate population hazard? If so, at what level?  
- We ask what genetic processes might shape individual hazards into having stretches that look Gompertz and old-age forms looking like plateaus.  
        
### Mutation Accumulation, Gompertz Hazards with Plateaus
- We turn to alternative models with plateaus in individual hazards, not just in aggregate population hazards.
- The genetic evolutionary theory of “mutation accumulation” suggests one story to account both for Gompertzian increases over a stretch of adult ages and for plateaus beyond them.
- We each inherit genetic variants or “alleles” in our DNA originating in mutations thousands of generations in the past.
- Picture, say, the time of the cave painters, 40,000 B.C., when people died in their 30s and 40s and 50s rather than their 70s, 80s, and 90s, losing some of their chance to bear and raise offspring.

#### Mutation-Selection Equilibrium
- Go back to basic stable (stationary) population theory. Write $\rho(a)$ for the size of the group of individuals (e.g. women) who carry a certain mutant allele indexed by the letter $a$.
- The NRR for members of the group is assumed to be $1 − S(a)$ for some small “selective cost” $S(a)$ due to effects of the “deleterious” allele.
- In the next generation, there are $\rho(a)(1 − S(a))$ daughters.
- There are also $\nu(a)$ new arrivals due to new mutations.
- The group keeps growing until it reaches equilibrium, when losses are balanced by new arrivals, and $\rho(a) = \rho(a)(1 − S(a)) + \nu(a)$.
- Then, $\rho(a)$ as a function of $S(a)$ is: $\rho(a)= \frac{\nu(a)}{S(a)}$
    
#### Deleterious Alleles with Age-Specific Effects
- In our setting, many deaths come from external threats regardless of age, a background level of “extrinsic mortality” with constant hazard $\lambda$.
- To keep the story as simple as possible, picture a mutant allele that has a small bad effect on survival only at an “age of onset” $a$. It raises the hazard of the individual who carries it by an amount $\delta$ in the age interval $a$ to $a + 1$.
- Picture a constant level of fertility from age 20 onward, set to make the NRR equal to 1 for women who carry no mutant alleles.
- Take, for example, $\lambda = 0.080$, $\delta= 0.002$, and $\nu(a) = 0.020/50$ for any $a \geq 20$. For each separate choice of age of onset $a$, find and plot the
equilibrium size $rho(a)$ as a function of $a$.
  - Here, we explore a genetic process which can generate a plateau in individual hazard functions. In particular, this is the case where a harmful mutant allele increases the hazard function of those that carry such allele with onset only in a single age interval.
```{r,echo=FALSE, warning=FALSE}
# Parameters
lambda   <-   0.080           #  background extrinsic hazard 
delta    <-   0.002           #  increment to hazard from mutant allele
epsilon  <-   0.0005          #  fixed cost 
nu       <-   0.200/50        #  mutation rate per site per generation

# Range of ages  
xxx      <-  c(0:50)          #  set of values of x, years past 20, up to 70
age      <-  20  +  xxx       #  age  

# Fertility level that makes baseline NRR equal to 1.
lx       <-  exp(-lambda*xxx) #  survivorship under background hazard 
fert     <-  1/sum(lx) #total fertility or constant level of fertility

# Loop over choices of an age interval a at which the mutant allele affects the hazard.

bigrho   <-  NULL             #  vector for holding values of rho  

for  ( a in seq(xxx))      { 
  
  hxa    <- lambda + 0*xxx + 0*epsilon        #  baseline hazard (no fixed cost)  
  hxa[a] <- hxa[a]  + delta                   #  add hazard increment in age interval a 
  
  Hxa   <- c(0, cumsum(hxa))[seq(hxa)]
  lxa   <- exp(-Hxa) 

  Sa    <- 1 - sum( fert*lxa)   # S(a): Selective cost
  rho   <- nu/Sa                # Rho(a)
  
  bigrho <- c(bigrho, rho)    
}   

```
  - $\rho(a)$ increases exponentially, this comes from the exponential survival. This graph tells us, the number of people that survive when the mutation is triggered at a given age. So at age 70, about 1400 people survive but if the mutation hit at age 20, nobody would survive. 

```{r,warning=FALSE,fig.cap='$\rho(a)$ as a function of age', fig.align='center', echo=FALSE}
par(mfrow=c(1,2))
plot(  age,  bigrho, ylab=expression(~rho(a)))
plot(  age,  log(bigrho), ylab =expression("log"~rho(a))  )
```

#### Allele Counts and Individual Hazards
- Suppose that the alleles carried by an individual are a random sample of the alleles present in the population, so that $\rho(a)$ can be reinterpreted as the mean number of alleles of type $a$.
- Then the hazard for an individual at age $a$ will look on average like the curve $\lambda + \rho(a) \times \delta$.
- Does this curve resemble a Gompertz hazard? What about a Makeham hazard, that is, a Gompertz hazard plus a constant?
  - This hazard looks like a Makeham hazard (Gompertz plus a constant).
```{r}
hxx  <-  lambda +  bigrho*delta  
```
```{r,warning=FALSE,fig.cap='Hazard after exposure to allele', fig.align='center', echo=FALSE}
plot ( age, hxx,  ylim = c(0,0.500) , ylab= "hazard")     
```
- The background intrinsic mortality $\lambda$ comes from the environment, not from the DNA. What might happen to $\lambda$ as we move from the time of the cave painters to the time of the moon landings?


#### Plateaus in Allele Counts
- Suppose, now, that each mutant allele raises the hazard by another small amount $\epsilon= 0.0005$ at all ages beyond 20, along with its special effect on raising the hazard by $\delta$ at age $a$.
- Find and plot the size $\rho(a)$ of the carriers of a at equilibrium under this new form of action.
- Is there an appearance of a plateau at high ages?
- Is it plausible to expect some fixed cost along with an age-specific effect from mildly deleterious mutant alleles?
  - Here is the case were a deleterious mutant allele imposes a fixed cost of size $\epsilon$ as well as adding an increment of size $\delta$ to the hazard in age interval $a$.
  - There appears to be a plateau on the survivors after the onset of the mutation at age $a$ when we add a fixed cost to the mutation, even if it is happening late in the reproductive cycle. 
```{r,warning=FALSE,fig.cap='', fig.align='center', echo=FALSE}
#Loop over choices of an age interval a at which the mutant allele affects the hazard with fixed cost

bigrho2   <-  NULL             #  vector for holding values of rho  
for  ( a in seq(xxx))      {
  
  hxa    <- lambda + 0*xxx + epsilon        #  baseline hazard plus fixed cost epsilon
  hxa[a] <- hxa[a]  + delta                   #  add hazard increment in age interval a 
  
  Hxa   <- c(0, cumsum(hxa))[seq(hxa)]
  lxa   <- exp(-Hxa) 
  
  Sa    <- 1 - sum( fert*lxa)       #   Selective cost including allele at age a 
  rho   <- nu/Sa                    #   Equilibrium count of mutation a 
  
  bigrho2 <- c(bigrho2, rho)
}      
```
```{r warning=FALSE,fig.cap='$\rho(a)$ as a function of age', fig.align='center', echo=FALSE}
par(mfrow=c(1,2))
plot(  age,  bigrho2, ylab=expression(~rho(a)))
plot(  age,  log(bigrho2), ylab =expression("log"~rho(a))  )
```

#### Questions for Further Study
- What about the effect of each allele on the selective cost of all the other alleles?
- Looking across species, under this account would we expect some relationship between the level of extrinsic background mortality and the steepness of the slope of a Gompertz increase in mortality with age?
- In our calculation we let mortality over the age of 50 continue to reduce the “NRR”. Why might survival beyond ages of childbearing have an effect on generational replacement?
- We carry in our DNA a load of mutant alleles shaped by natural selection over hundreds and thousands of generations. Why might it be that effects of those alleles that were once lethal in mid-adult life could now be influencing rates of mortality late in life for us?

## Key points 
- Plateaus are significant information for demographers who try to predict future progress against old-age mortality. Gompertz discouragement – Plateau encouragement
- Gamma-Gompertz models offer one explanation for plateaus, but no explanation for Gompertz acceleration, which is simply assumed.
-In frailty models, plateaus come out of the specialness of survivors. The mathematics treats mixtures of survivorship. 
- Evolutionary genetic models aim to explain both Gompertz acceleration and old-age plateaus.
- In the mutation-accumulation story, plateaus are present in the hazard functions of individuals, shaped by inherited genetic load.
- In demography, survivorship is ALWAYS an exponential function of cumulative hazards.
- In genetic stories, this exponential is the source of the exponential in the Gompertz formula.
- The study of hazards at extreme age may inform our understanding of hazards at younger ages.  

<!--chapter:end:05-plateaus.Rmd-->

# Tempo

## Outline

- Introduction and a tempo simulation
- Bongaarts and Feeney's formula
- An application to the United States
- Two Americas?
- EM algorithm for unmixing mixtures
- An application to two Americas.

Additional resources:

  - @sullivan2005age: An early paper (by a Berkeley Demog student!) focusing on ﬁrst birth hazards.
  - @burkimsher2017evolution: A descriptive paper, which you can mostly skip. But see especially section 7, where she argues that her ﬁndings contradict Sullivan for the United States.
  - @hastie2009elements: A textbook example of expectation-maximization algorithm applied to mixture of two normals in Section 8.5.
  - Victor Lavrenko. Youtube video : "EM algorithm: how it works" <https://www.youtube.com/watch?v=REypj2sy_5U>. 
  - @bongaarts1998quantum

## Introduction

What we see is superficial. Heterogeneous models reveal what's "really" going on. (Or do they?). Until the past sections, population hazards mislead. However in this section, homogeneous fertility misleads.
  
We now reverse perspectives:

  - We see differences we see in genotypes, in lineages, in names.
  - These could be due to "real" differences (heterogeneity).
  - But they could also be due to luck: everyone is the same but stochastic outcomes differ.
  - Our models of individual-level randomness will have predicted dynamics, which are themselves interesting but can also be used as a "null" to compare to observations.

### Fertility postponement, a very simple example

  Baseline
  
  - A population has a history of 1 birth per day
  - When women turn age 25, they have a daughter.
  - This gives us a constant stream of births, 365 per year.
  
  Postponement
  
  - Starting on Jan. 1, 2020, everyone postpones childbearing an additional month, until they are aged 25 1/12.
  -  How many births will there be in 2020?
  -  How many births in 2021?
  
  As everybody postpones childbearing for a month, then the first birth of 2020 occurs on February 1st so that 31 babies have not been born by then. This means that during 2020 there are $365-31=334$ births. For this year, births and total fertility rate decrease. However, the postponement doesn't affect the birth stream in 2021 as there will still be one birth per day, even if in the absence of postponing these some of the births should've occured in 2020.
  
### Continuous postponement, a shiny simulation
  To answer these questions, we can use the following shiny app. 
```{r echo=FALSE}
knitr::include_url('https://shiny.demog.berkeley.edu/josh/tempo/', height = '600px')
```


- $R(t)$ Cumulative postponment

- $r(t)$ Incremental postponement $r(t) = R'(t)$


  What is a formula for recovering original birth stream?
  \begin{align}
  \hat{B}_{orig} &= B_{obs} \times (1 + R'(t)) \\
   \text{or}\\
  \hat{B}_{orig} &= B_{obs} \times 1/  \left[1 - R'(t)\right]?
  \end{align}

  Note: this idea of ``recovering original'' is one way to think
  about tempo adjustment.
  
  We can think of the original version as that which should have ocurred in the absence of postponement. Intuitively, it should be higher than the observed birth stream, but how much larger? The table below shows an example of how to recover the original births where each formula refers to the equations above.

```{r, echo=FALSE, message=FALSE}
  library(tidyverse)
  rt <- seq(0.1,0.4,0.1)
  b_obs <- c(91,84, 77, 72 ) 
  b_orig1 <- b_obs*(1+rt)
  b_orig2 <- b_obs/(1-rt)
  b_orig <- tibble(rt,b_obs, b_orig1, b_orig2)
  names(b_orig) <- c("r(t)","B(obs)", "B(orig) formula 1", "B(orig) formula 2")
  knitr::kable(b_orig)
```
  
  The correct formula is equation 2. **NOTE: is this correct? If so, why?** 
  
## Period Shifts: Bongaarts and Feeney's model

In a bigger microsimulation

  - Each period will have births across a range of ages
  - We'll randomly generate the original planned birthdays
  - Then we'll shift by a continuous function $R(t)$.
  

The birth rate of women, $f(a,t)$, aged $a$ in period $t$ is: 
  $$
  f(a,t) = f_0(a - R(t)) (1- R'(t)) q(t)
  $$
  
Where

- $f_0$:  constant baseline schedule (can be normalized to sum to 1).
- $q(t)$: period intensity parameter: "quantum"
- $R(t)$: cumulative shift.
  

An example

$$
f(a,t) = f_0(a - R(t)) (1- R'(t)) q(t)
$$
  
  
- $R_{2019} = 3$
- $R'_{2019} = .1$
- $q(2019) = 1$
  

Give an expression for $f(28,2019)$.

$$ \begin{aligned} 
 f(28,2019) &= f_{0}(28-3)\times(1-0.1)\times(1) \\
  & = (0.9)f_{0}(25)
 \end{aligned}
$$  Therefore, the fertility rate for a 28 year-old in 2019 would be 0.9 of the baseline fertility rate at age 25. Another way of thinking about it is that had there been no postponement, the fertility rate that we are observing in 2019 for this 28 year-old would actually be similar to that of a 25 year old. 

Small changes in the stock can have huge effect in the flows.


### A derivation: due to Rodriguez *NOTE: not sure what the reference is here*

Assume no quantum effects (i.e, no $q(t)$). Take a cohort with cumulative fertility

  $$
  F_0(a) = \int_0^a f(x) \,dx
  $$

  Now put in shifts so that observed fertility is from an age $R(t)$ years earlier. ("28" is the new "25"!)
  $$
  F(a,t) = F_0(a - R(t))  = F_0(a - R(c + a))
  $$
  
  Differentiate with respect to age (which for a cohort is also time $t$), using chain rule
  
  $$ \begin{aligned}
  \frac{d }{dt}F(a,t)= f(a,t) &= F'_{0}(a-R(c+a))\times(1-R'(c+a)) \\
  &= f_0(a - R(t)) \left[1 - R'(t)\right]
  \end{aligned}$$

Let's re-notate our constant quantum result
  $$
  f_0(a,t | R(t) ) = f_0(a - R(t)) \left[1 - R'(t)\right]
  $$
  Then we can incorporate period quantum on the shifted surface:
  $$
  f(a,t) = f_0(a,t | R(t) ) q(t) = f_0(a - R(t)) \left[1 - R'(t)\right]q(t)
  $$

  Note: If we vary quantum before shifts, then $q(t)$ will bleed into neighboring years. (a small effect, but makes model
  messier). **NOTE: I don't understand this, could you explain Josh?**


**Tempo-adjusted TFR:** counter-factual, TFR in absence of timing changes

  $$
  TFR(t) = \int_0^\infty f(a,t) \, da
  $$

  Substituting our shifted birth rates with quantum
  $$
  \begin{aligned}
  TFR(t) &= \int_0^\infty f_0(a - R(t)) \left[1 - R'(t)\right]q(t) da \\
  &=\left[1 - R'(t)\right]q(t) \int_0^\infty f_0(a - R(t))  da \\
  &= TFR_0 \left[1 - R'(t)\right] q(t)\\
  \end{aligned}
  $$
  Without loss of generality, define $TFR_0 = 1$, then 
  
  $$
  q(t) =   \frac{TFR(t)} {1 - R'(t)} \equiv TFR^*(t) 
  $$
  The observed $TFR(t)$ deflated by the rate of change ($1 - R'(t)$) is the BF formula.


How do period schedules change?

  $$
  f(a,t) = f_0(a - R(t)) \left[1 - R'(t)\right]q(t)
  $$


  What is
  $$
  \begin{aligned}
  {\partial \over \partial t} \log f(a,t) &= ?\\
  \\
  & = \frac{d}{dt}log(f_{0}(a-R(t))) + \frac{d}{dt}log(1-R'(t)) + \frac{d}{dt}log(q(t)) \\
  & = \frac{-f'_{0}(a-R(t)) R'(t)}{f_{0}(a-R(t))} - \frac{R''(t)}{1-R'(t)}+ \frac{q'(t)}{q(t)}
  \end{aligned}
  $$
  If we sketch this, where the age is on the x-axis, then the last two components affect the intercept of the curve. If births are postponed, $R'(t)>0$ but if they are actually advanced then $R'(t)<0$, which affects the slope of $ {\partial \over \partial t} \log f(a,t)$.

**Uniform shifts**

- BF model assumes all ages shift by $R(t)$.
- BF model assumes all ages rise or fall by same quantum $q(t)$
- Violating these assumptions means change in mean age will not just reflect "tempo".
- Example: What happens if people have fewer higher order births?
  
**BF recommendation for achieving uniformity**

  Separate estimates for each birth order, and then combine:
  $$
  TFR^*(t) = \sum_i TFR_i^*(t) = \sum_i {TFR_i(t) \over 1 - r_i(t)}
  $$

This will protect against order-specific quantum effects.
  

## An Application to the United States

Tempo adjustment of US fertility using HFD data using Bongaarts-Feeney formula:

1. Read in data and format into an array. 
   Below we show the period fertility rates for all parities at each age for 1933 and 1934 of US women. 
```{r, message=FALSE}
library(data.table)
library(dplyr)
library(knitr)

source("https://raw.githubusercontent.com/josh-goldstein-git/random_demography/master/bookdown-master/codes/tempo_functions.R")
source("https://raw.githubusercontent.com/josh-goldstein-git/random_demography/master/bookdown-master/codes/utility_functions.R")

## age specific fertility rates by birth order for all countries and times
## RR means "rectangles" on Lexis surface
dt <- fread("https://raw.githubusercontent.com/josh-goldstein-git/random_demography/master/bookdown-master/data/zip_w/asfrRRbo.txt", showProgress = FALSE)

dt <- dt[Code == "USA"] ## keep only US
dt <- dt[Age %in% 15:49] ## keep only ages 15 to 49

## put all order fertility into a matrix
fat <- dt[, xtabs(ASFR ~ Age + Year)] # age (rows) by cohort year (column) matrix of ASFR
fat <- as.matrix(unclass(fat)) 
fat1 <- dt[, xtabs(ASFR1 ~ Age + Year)] #age specific fertility rates for parity = 1 
fat2 <- dt[, xtabs(ASFR2 ~ Age + Year)]
fat3 <- dt[, xtabs(ASFR3 ~ Age + Year)]
fat4 <- dt[, xtabs(ASFR4 ~ Age + Year)]
fat5p <- dt[, xtabs(ASFR5p ~ Age + Year)]
year.vec <- colnames(fat)
age.vec <- rownames(fat)
parity.vec <- c("all", 1:5)
fat.array <- array(NA, dim = c(nrow(fat), ncol(fat), length(parity.vec)))
dimnames(fat.array) <- list(age.vec, year.vec, parity.vec)
fat.array[,,"all"] <- fat
fat.array[,,"1"] <- fat1
fat.array[,,"2"] <- fat2
fat.array[,,"3"] <- fat3
fat.array[,,"4"] <- fat4
fat.array[,,"5"] <- fat5p

kable(fat.array[,1:2,"all"], caption="An extract of period age-specific fertility rates") 
``` 

2. Fit bongaarts feeney without birth order
```{r}
tfr.vec <- colSums(fat)          # total fertility rate for each cohort
mu.vec <- apply(fat, 2, get.mean) # mean age at childbearing
rt.vec <- center.diff(mu.vec)     # increments of postponement
adj.tfr.vec <- tfr.vec / (1 - rt.vec) # Assuming no quantum effect, tempo-adjusted TFR
```

```{r, echo=FALSE, fig.cap='Adjusted TFR',fig.height=8}
par(mfrow = c(3,1))
plot(names(mu.vec), mu.vec, xlab = "Year", ylab="Mean age of childbearing")
plot(names(mu.vec), rt.vec, xlab = "Year", ylab= "Shifts")
abline(h =0)
plot(year.vec, tfr.vec, type = "l", xlab = "Year", ylab= "TFR")
lines(year.vec, adj.tfr.vec, lty = 2)
abline(v = c(1945, 2008))
legend("topright", c("Observed TFR", "Tempo-adjusted TFR"), lty= c(1,2))
```

- We see fertility since 1980 has been depressed by postponment
- We see weird dynamics around end of WW2 and great recession.
- What's going on? Here is a closeup

```{r, echo=F, fig.cap='Observed and tempo-adjusted TFRs'}
par(mfrow = c(1,1))
plot(year.vec, tfr.vec, type = "l", xlab = "Year", ylab= "TFR") # Observed TFR
lines(year.vec, adj.tfr.vec, lty = 2) # Tempo-adjusted TFR
abline(v = c(1945, 2008))
abline(h = seq(1.5, 4, .1), col = "grey", lty = 3)
legend("topright", c("Observed TFR", "Tempo-adjusted TFR"), col= c("black","red"), lty=c(1,1))

## can also use function to fit
adj.tfr.vec.from.fun <- bf.fit.simple(fat)$tfr.star
lines(year.vec, adj.tfr.vec.from.fun, col = "red")
```

Now let's look at turbulence around WWII
```{r, echo=FALSE,fig.cap='Change in fertility rates around WWII', fig.height=6}
par(mfrow = c(2,2))
plot(age.vec, fat[,"1944"], type = "l", ylim = c(0, .23),
     ylab = "f(a)",
     xlab = "age a"
     )
lines(age.vec, fat[,"1945"], type = "l", col = "red")
lines(age.vec, fat[,"1946"], type = "l", col = 'orange')
lines(age.vec, fat[,"1947"], type = "l", col = "blue")
legend("topright",
       legend = 1944:1947,
       col = c("black", "red", "orange", "blue"),
       lty = 1)
title("Age specific fertility")
##
plot(1943:1947, mu.vec[paste(1943:1947)],
     ylab = "mu(t)",
     xlab = "t",
     col = c("black", "black", "red", "orange", "blue"),
     pch = 19)
title("Mean ages")
##
plot(1943:1947, rt.vec[paste(1943:1947)],
     ylab = "r(t)",
     xlab = "t",
     col = c("black", "black", "red", "orange", "blue"),
     pch = 19)
title("Changes in mean, centered")
##
plot(1943:1947, tfr.vec[paste(1943:1947)],
     ylab = "tfr",
     xlab = "t",
     ylim = c(1, 4),
     type = "l")
lines(1943:1947, adj.tfr.vec[paste(1943:1947)],
      lty = 2)
title("TFR and adjTFR")
legend("topright", c("Observed TFR", "Tempo-adjusted TFR"), lty=c(1,2))

``` 

From 1945 to 1946, fertility goes up a lot, but more at younger ages. So mean goes down. BF adjustment over-compensates, and has quantum declining.

What's happening from 1944-45?

3. Fit bongaarts feeney with birth order

```{r}
out <- bf.fit(fat.array)  #function to obtain BF tempo-adjusted TFR
adj.tfr.bo.vec <-  out$tfr.star.out[, "bf.tfr.star"] # sum of parity specific tempo-adjusted TFRs
```
```{r, echo=FALSE,fig.cap='TFR by parities', fig.height=6}
par(mfrow = c(1,1))
plot(year.vec, tfr.vec, type = "l", lwd = 2, xlab="Year", ylab= "TFR")
lines(year.vec, adj.tfr.vec, lty = 2)
lines(year.vec, adj.tfr.bo.vec, lty = 1, lwd = 2, col = "red")
legend("topright",
       c("tfr", "tfr* (all parities)", "tfr* (sum of parities)"),
       col = c("black", "black", "red"),
       lty = c(1, 2, 1),
       lwd = c(2,1,2))
```
4. Use HFD data to verify adjusted TFR
  HFD uses a tempo-adjusted TFR that is the sum of the parity specific adjusted BF TFR.
```{r}
## let's check against hfd
hfd.adj.dt <- fread("https://raw.githubusercontent.com/josh-goldstein-git/random_demography/master/bookdown-master/data/zip_w/adjtfrRR.txt", showProgress = FALSE, skip = 2)
hfd.adj.dt <- hfd.adj.dt[Code == "USA"]
```

  We find that:  
  
  - Taking birth order into account smooths out WW2 turbulence (large fluctuations) but increases the variation during the Great Recession.  
  - The baby boom appears to initially be even greater than observed when we take into account birth order, but then it fall more than observed. 
  - Finally, the baby bust was not as bad as it seemed.

```{r, echo=FALSE,fig.cap='TFR by parities using HFD data', fig.height=6}
plot(year.vec, tfr.vec, type = "l", lwd = 2, xlab="Year", ylab= "TFR")
lines(year.vec, adj.tfr.vec, lty = 2)
lines(year.vec, adj.tfr.bo.vec, lty = 1, lwd = 2, col = "red")
lines(hfd.adj.dt$Year, hfd.adj.dt$adjTFR, col = "blue")
legend("topright",
       c("tfr", "tfr* (all parities)", "tfr* (sum of parities)", "HFD adj TFR"),
       col = c("black", "black", "red", "blue"),
       lty = c(1, 2, 1, 1),
       lwd = c(2,1,2,2))

```

### Conclusions

- Baby boom smaller if we account for "pre-ponement".  
- Fertility lull in 1970s and 80s disappears if we account for
    "postponement". 
- Birth order disaggregation improves estimates of shifts from
    changes in mean age
- What happened with the recession?

## Two Americas

  Let's look at births (all orders). Here we have some animations of the ASFR over time. 
```{r, message=FALSE, warning=FALSE, fig.height=7}
my.year.vec <- 1975:2017
# library(devtools)
# install_github("dgrtwo/gganimate")
# install.packages("transformr")
# install.packages("animation")
library(gganimate)
library(data.table)
library(mixtools)
library(ggplot2)

source("https://raw.githubusercontent.com/josh-goldstein-git/random_demography/master/bookdown-master/codes/tempo_mixed_functions.R")
source("https://raw.githubusercontent.com/josh-goldstein-git/random_demography/master/bookdown-master/codes/tempo_functions.R")

# plot1 <- ggplot(data=dt[Year %in% my.year.vec,2:4], aes(x=Age, y= ASFR)) + 
#   geom_line(aes(group=Year))+
#   transition_time(Year)+
#   labs(title = "Year: {frame_time}")

# anim_save("plot1.gif", plot1)
```
![](plot1.gif)

During the earlier years, the mean childbearing age seems to be in the early 20s. However with time, the AFR seems to become bimodal. So what is happening here? Are there two underlying groups of women that experience different fertility rates. Now fit mixing model and redo the animation.

**NOTE: Not sure of all the lines in this mixture code. Josh, could you add some notes please?**
```{r mixture1, results='hide'}
## takes few minutes to run
my.fat <- fat[, paste(my.year.vec)]
out <- get.mixed.tfr.star(my.fat)
##
out.all <- out

#Mixture simulation 
if (0) {
mu.mat <- get.coefs.mixed(out.all$fert.fit.list.variable.sigma)$mu.mat
lambda.mat <- get.coefs.mixed(out.all$fert.fit.list.variable.sigma)$lambda.mat
sigma.mat <- get.coefs.mixed(out.all$fert.fit.list.variable.sigma)$sigma.mat
}
mu.mat <- get.coefs.mixed(out.all$fert.fit.list)$mu.mat
lambda.mat <- get.coefs.mixed(out.all$fert.fit.list)$lambda.mat
sigma.mat <- get.coefs.mixed(out.all$fert.fit.list)$sigma.ma

#Temporary comment out until we figure out what these graphs actually are
# matplot(my.year.vec, t(mu.mat))
# abline(v = 2015)## problem here
# points(c(2015, 2015), c(21.5, 30.3))
# 
# matplot(my.year.vec, t(lambda.mat))
# abline(v = 2015)## problem here
# points(c(2015, 2015), c(21.5, 30.3))

## interpolate 1915

colnames(lambda.mat) <- my.year.vec
colnames(mu.mat) <- my.year.vec
lambda.mat[,"2015"] <- (lambda.mat[,"2014"] + lambda.mat[,"2016"])/2
mu.mat[,"2015"]     <- (mu.mat[,"2014"]     + mu.mat[,"2016"])/2
``` 

  Now we create the animation with the normal distributions inside as well as the observed ASFR.
```{r mixture2, fig.height=7}
# Obtaining different ASFRs per group (1 and 2)
my.dt <- dt[Year %in% my.year.vec,]

for (i in 1:length(my.year.vec)){
    my.year <- my.year.vec[i]
    year.vec <- colnames(my.fat)
    fx <- my.fat[,paste(my.year)]
    # fx <- fx/sum(fx)
    x <- as.numeric(names(fx))
    ## par(mfrow = c(1,1))
    # plot(x, fx,
    #      ylim = c(0, .3),
    #      ylab = "normalized fx")
    s <- year.vec == my.year
    this.tfr <- sum(fx)
    fx1.hat <- dnorm(x, mean = mu.mat[1,s], sd = sigma.mat[1,s]) *
        lambda.mat[1,s]* this.tfr
    # lines(x, fx1.hat, col = "red")
    fx2.hat <- dnorm(x, mean = mu.mat[2,s], sd = sigma.mat[2,s]) *
        lambda.mat[2,s] * this.tfr
    # lines(x, fx2.hat, col = "blue")
    # lines(x, fx1.hat + fx2.hat)
    # title(last.year)
    my.dt <- my.dt[Year==my.year, fx := fx]
    my.dt <- my.dt[Year==my.year, fx1 := fx1.hat]
    my.dt <- my.dt[Year==my.year, fx2 := fx2.hat]
    my.dt <- my.dt[Year==my.year, fx1fx2 := fx1.hat + fx2.hat]

}

labels <- c("Observed" = "black", "Mixture 1" = "blue", "Mixture 2" = "red", "Sum of mixtures"="orange")

# plot2 <-
#   ggplot(data=my.dt, aes(x=Age)) + 
#   geom_line(aes( y= ASFR, group=Year, color = "Observed"))+
#   geom_line(aes( y= fx1, group=Year, color = "Mixture 1"))+
#   geom_line(aes( y= fx2, group=Year, color = "Mixture 2"))+
#   geom_line(aes( y= fx1fx2, group=Year, color = "Sum of mixtures"))+
#   labs(color = "Legend") +
#   scale_color_manual(values = labels)+
#   theme_bw()#+
#   transition_time(Year)+
#   labs(title = "Year: {frame_time}")

# anim_save("plot2.gif", plot2)


```
![](plot2.gif)
  The animation shows how the distributions operate under the AFSR curve. There could be two separate normal distributions with distinct mean ages of childbearing.

  Let's do tempo adjustment:**NOTE: Help! I don't know how to interpret these graphs**
```{r, fig.height=5}
rt.mat <- t(apply(mu.mat, 1, center.diff))
tfr.vec <- apply(my.fat, 2, sum)
tfr.mat <- lambda.mat * tfr.vec
par(mfrow = c(1,2))

matplot(my.year.vec, t(tfr.mat), ylim = c(0, 3))
tfr.star.mat <- tfr.mat / (1 - rt.mat)
matplot(my.year.vec, t(tfr.star.mat), ylim = c(0,3))

tfr.star.vec <- colSums(tfr.star.mat)

par(mfrow = c(1,1))
plot(my.year.vec, tfr.vec, type = "l",
     ylim = c(1, 3))

lines(my.year.vec, tfr.star.vec, lty = 2)
```

### Mixture
  Let's look at 1st births, again as if their are two latent groups: $A$ and $B$.  (These could be "early moms" / "late moms", non-college / college, pre-marital / marital, lower-class / upper class, $\ldots$)
```{r}
library(mixtools)
  ## simulate 2 normals 
  N <- 1000
  x1 <- rnorm(N, mean = 22, sd = 3) ##
  x2 <- rnorm(2*N, mean = 30, sd = 4)
  ## combine them
  x <- c(x1,x2)
  ## use EM to infer mixture
  out <- normalmixEM(x,
                     lambda = c(.5, .5),
                     mu = c(15, 35),
                     sigma = c(5,5))
  print(out$mu)
  print(out$sigma)
  print(out$lambda)
```
  Seems to work great.

```{r, fig.height = 4}
  ages <- 10:49
  dens1 <- dnorm(x = ages, mean = out$mu[1], sd = out$sigma[1]) * out$lambda[1]
  dens2 <- dnorm(x = ages, mean = out$mu[2], sd = out$sigma[2]) * out$lambda[2]
  par(mfrow = c(1,1))
  hist(x, probability = T, col = "grey")
  lines(ages, dens1, col = "red", lwd = 2)
  lines(ages, dens2, col = "blue", lwd = 2)
  lines(ages, dens1 + dens2, col = "black", lwd = 2)
``` 

### An algorithm for tempo adjustment of mixtures
  
- Fit normal mixture to each year.
- Refit using constant variance (average). This assures shape invariance of each component, fulfilling BF assumption.
- Estimate BF separately for $A$ and $B$, and combine.



- tempo_mixed_results_figure.pdf **NOTE: not sure about this...**
  

## Conclusions 
  
  - Postponement dilutes period births, lowers TFR
  - Tempo-adjustment tries to ``put births back in''
  - Changes in mean work fine if ``shape'' doesn't change
  - Shape can change through heterogeneity
  - With strong assumptions, we can identify heterogeneity
  - Declining quantum for young and postponement for old appears
    to be the story
  
### Caveats
  
  - Who are these latent groups? Do you start out in one and end up in the other? Do you stay in one your whole life?
  - How do we project forward?
  - Can we use other indicators (e.g., social class, education,
    marriage) to get same results?

<!--chapter:end:06-tempo.Rmd-->

# Branching Processes 

## Outline 
  
  - The Galton-Watson-Bienaym\'e Process: Motivation
  - Simulating a branching process
  - Moment generating functions
  - Extinction probabilities
  - The distribution of offspring of all generations
  - A tractable offspring distribution
  
Additional resources:

  - @grinstead2006: An intermediate/advanced undergraduate textbook with good section on Branching Processes, with references to Harris and Keyﬁtz below.
  - @harris1964theory: Classic reference, but readable if you take your time. We will only read chapter 1 and will only consider the very easiest material.
  - _Surname Extinction: When will we all be "Smiths"?_ <https://www.youtube.com/watch?v=5p-Jdjo7sSQ>: Popular science video providing good motivation, but without authority.
  - _Is Your Surname about to Go Extinct?_ <https://blogs.ancestry.com/cm/is-your-rare-surname-about-to-go-extinct/>:A brief blog entry, mentionng of "endangered names".
  - @keyfitz1968: Another introduction to branching process with the details of the empirical example cited by Grinstead and Snell.

## Motivation

- Until now, we've focused on the hidden structures of heterogeneity.
- Now, we're switching gears:
  - Stochastic not deterministic
  - In small populations, randomness matters. (Even when risks are homogeneous.)
  - We will look at branching processes ("parents producing children"), next Fisher-Wright ("children choosing parents"), and then historical reconstruction from contemporary diversity ("coalescent").

### Very brief history of Branching Processes 
  
  - Bienaymé's lost notes
  - Old motivation: Galton and Watson's: to see if elites were dying out because of "degeneration" 
  - Contemporary motivation: evolution and neutral genetic change. What is the chance that a mutant will survive?
  - War-time motivation: to see how to build the bomb (chain reactions)
  - Sociological: Anywhere "incipient dynamics" matter (will all of S. Korea be "Kim"?)
  - Can we get variance of reproductive success from name disambiguation?
  - It will give us a headstart on other (less realistic but easier) "drift" models.
  
### Applicability to the Coronavirus? Yes and no.

  - Perhaps the beginning, with first few cases.
  - But once scale gets large, we'll see that deterministic dynamics take over.
  - One lesson: beyond $R_0$.
  
### Simulated examples and the questions they raise

  Here are the chances, $p_k$, that the first carrier passes on the virus to $k$
  people
  
+----+------+
| $k$| $p_k$|
+====+======+
| 0  | .3   |
+----+------+
| 1  | .4   | 
+----+------+
| 2  | .3   |
+----+------+

  - What is $R_0$, (aka $m$)? This is the expected number of infected cases: 
  $$\begin{aligned}
  R_{0} & = 0\times 0.3 + 1\times 0.4 + 2\times 0.3 \\
  & = 1 
  \end{aligned}$$ **NOTE: is this correct?**
  
  -  Now assume that people will infect $k$ other people based on a random number, for instance the last 4 digits of their phone number. 
  

+-----+-------+--------+
| $k$ | $p_k$ | digits |
+=====+=======+========+
| 0   | .3    | 0-2    |
+-----+-------+--------+
| 1   | .4    | 3-5    |
+-----+-------+--------+
| 2   | .3    | 6-9    |
+-----+-------+--------+

  -  Let's diagram one chance outcome, using my number "(xxx) xxx-9056". As each of the four digits is associated with a probability of infecting more people, they can be seen as different the carrier generations. For instance, the first carrier has a probability $p_k=0.3$ of infecting $k=2$ other people. So that first carrier may infect 2.7 people, (3 for simplicity). Each of these three people will have a $p_k=0.2$ of each infecting $k=0$ people. Since they don't infect anybody then there are no further carrier generations. In the table, we include their rows but the expected number of infected people is still 0 because the spreading stopped in generation 2.  

+--------------+---------+-------+-------------------+
| carrier      | random  |       | expected          | 
| generation # | digit   | $p_k$ | # infected        | 
+==============+=========+=======+===================+
| 1            | 9       | 0.3   |$2\times 0.3 = 2.7$|
+--------------+---------+-------+-------------------+
| 2            | 0       | 0.2   |$0\times 0.3 = 0$  |  
+--------------+---------+-------+-------------------+ 
| 3            | 5       | 0.4   | 0                 |
+--------------+---------+-------+-------------------+
| 4            | 6       | 0.4   | 0                 |
+--------------+---------+-------+-------------------+

**NOTE: does this sound reasonable? **

### What is a (Bienayme)-Galton-Watson branching process?

  - $p_k$: Each individual in each generation reproduces independently, following same offspring distribution, with $p_k$ as the probability of having $k$ offspring.
  - $Z_n$: The si$Z$e of the $n$'th generation $Z_n$. ($Z_1 \equiv 1$)
  - $p_0 > 0$: Some non-zero probability of no children.
  - Variance: None of the $p_k$ are 1


![Galton's original question](./images/galton.png)

Some questions
  
  - What is the chance $d$ of eventual extinction (no "outbreak")?
  - Or, what is the distribution of surviving family sizes?
  - What are the aggregate properties of many branching processes? (Mean growth, variance, time-paths, eventual size)?
  
## Simulations of parents having children
  
```{r, cache= T} 
  k = 0:2 #number of possible children
  p0 = .3; p1 = .3; p2 = .4; 
  p_k = c(p0, p1, p2)  #probabilities of having k children
  Z1 = 1               #initial cohort size
  set.seed(9)
  kids.of.Z1 = sample(x = k, size = Z1, replace = T, prob = p_k)
  Z2 = sum(kids.of.Z1)
  kids.of.Z2 = sample(x = k, size = Z2, replace = T, prob = p_k)
  Z3 = sum(kids.of.Z2)
  kids.of.Z3 = sample(x = k, size = Z3, replace = T, prob = p_k)
  Z4 = sum(kids.of.Z3)
```
```{r, message=FALSE,  echo=FALSE}
  gen_size <- c(rep(Z1, 1), rep(Z2, Z2), rep(Z3, Z3), rep(Z4, Z4))
  gen <- c(rep(1, 1), rep(2, Z2), rep(3, Z3), rep(4, Z4))
  person <- c(1, seq(2.1,by=0.1, to=2 + Z2*0.1), 
              seq(3.1,by=0.1, to=3 +Z3*0.1),
              seq(4.1,by=0.1, to=4 +Z4*0.1))
  total_children <- c(kids.of.Z1, kids.of.Z2, kids.of.Z3, rep(0, Z4))

  library(tidyverse)
  growth <- tibble(gen, person, total_children, gen_size)
  names(growth) <- c("Generation #", "Person ID", "Total children", "Generation size")
  knitr::kable(growth)
```

  - Here's a more visual representation. 

```{r, echo= FALSE,fig.cap=''}
# install.packages('DiagrammeR')
DiagrammeR::grViz("
                  digraph rmarkdown{
                  1 ->{2.1 2.2}
                  2.1->{3.1 3.2}
                  2.2->{3.3 3.4}
                  3.1->{4.1 4.2}
                  3.2->{4.3}
                  3.3->{4.4 4.5}
                  3.4->{4.6 4.7}
                  }")
```

  - However, we can extend the number of generations in a function:

```{r}
  #A function   
  branch <- function(n_max = 30, pk = c(p0, p1, p2), Z1 = 1)
  {
      Z.vec <- rep(NA, n_max) # n_max handles the total number of generations
      Z.vec[1] <- Z1
      for (i in 1:(n_max-1))
      {
          Z.vec[i+1] <- sum(sample(x = k,
                                   size = Z.vec[i],
                                   replace = T,
                                   prob = p_k))
      }
      return(Z.vec) # returns a vector of the number of children at every generation
  }
```
  - Sometimes generations die out:  
```{r}
  set.seed(19) ; branch()
  set.seed(99) ; branch()
```
  - Let's see what happens with 20 trials (up to 30 generations). Not all generations go extinct here. 
```{r }
  n_trials = 20
  n_gen = 30
  k = 0:2 #number of possible children
  p0 = .3; p1 = .3; p2 = .4; 
  p_k = c(p0, p1, p2)  #probabilities of having k children
  Z1 = 1               #initial cohort size
  
  Z.mat <- matrix(NA, n_trials, n_gen)
  set.seed(131) 
  for (i in 1:n_trials)
      Z.mat[i,] <- branch(n_max = n_gen)
```
```{r, echo=FALSE, fig.cap='Generation size'}  
  matplot(t(Z.mat),
          type = "l", lty = 1, ylab = "Zn", xlab = "n")
```
  - How many survive (out of 20)? log-scale
```{r, echo=FALSE, fig.cap='Generation sizes (log-scale)'}
  suppressWarnings(matplot(t(Z.mat), log = "y", type = "l", lty = 1, ylab = "Zn", xlab = "n"))
  surviving = ifelse(Z.mat[,n_gen] == 0, "extinct", "survive")
  foo <- prop.table(table(surviving))
  print(prop.table(table(surviving)) )
```
  - How would you discribe the time path of the surviving lines? Let's extend the number of generations to observe any long-term trends.
```{r, fig.height = 4}
  n_trials = 20; n_gen = 100
  Z.mat <- matrix(NA, n_trials, n_gen)
  set.seed(131) 
  for (i in 1:n_trials)
      Z.mat[i,] <- branch(n_max = n_gen)
```
```{r, echo=FALSE, fig.cap='Generation sizes (log-scale) over many generations'}
  suppressWarnings(matplot(t(Z.mat), log = "y", type = "l", lty = 1, ylab = "Zn", xlab = "n"))
```
  - What does this remind you of? (Hint: "Leslie"). (See Harris figure) **NOTE: Josh, which Harris figure?**

## The Probability Generating Function: Our mathematical tool

  - "Extinction" vs "breakout"
  
    - We see that in a super-critical ($m > 1$) branching process, if a line can survive a few generations and reach a large enough size, it will grow exponentially.
    - What happens if $m < 1$, if $m = 1$? Discuss.
  $$
  h(z) = p_0 + p_1 z + p_2 z^2 + \ldots
  $$
    - The PGF "keeps book" on the probabilities. The chance of $k$ is the coefficient on $z^k$.  
    - $h(0)= p_{0}$   
    - $h(1)= p_{0} + p_{1}$   
    - $h'(1)= p_{1} + 2p_{2}$

  - The story of two brothers. A father has two sons. The probability generating function of their children combined is:
  $$
  h(z)^2 = (p_0  + p_1 z + p_2 z^2) \times (p_0  + p_1 z + p_2 z^2)
  $$
    - If we multiply it out we get:
 $$
 \begin{aligned}
 \left[h(z)\right]^2  = & p_0^2  + p_0 p_1 z + p_0 p_2 z^2 +\\
 & p_1 z p_0 + p_1^2 z^2 + p_1 p_2 z^3 + \\
 & p_2 z^2 p_0 + p_2 z^3 p_1 + p_2^2 z^4 \\
 & = p_0^2 + (2p_0 p_1 )z + (2p_0 p_2 + p_1^2)z^2 + (2p_1 p_2)z^3 + p_2^2 z^4
 \end{aligned}
 $$
 
    - The coefficients on $z^0, z^1, \ldots$ tell us the probability that the sons will have $k=0,1,2,3,4$ sons.

  - What is the probability generating function for the distribution of grandsons?
    - A man has two sons, with probability $p_2$, so PGF in that case is $p_2 [h(z)]^2$.
    - But let's sum over all possible numbers of sons.
    $$
    p_0 + p_1 h(z) + p_2 [h(z)]^2 + p_3 [h(z)]^3 + \ldots
    $$
    - This is the cumulative probability of drawing $k$ children within a generation of size $Z$ 
    - Which is?
    $$
    h(h(z))
    $$
  - Can show PGF for the n'th generation is
    $$
    h(h(h ... \mbox{$n$ times}  h(z)))  = h_n(z)
    $$
  - For instance, let's get $h_2(z) = h(h(z))$ for $ h(z) = p_0 + p_1 z + p_2 z^2$
  $$\begin{aligned}
  h(h(z)) = &  p_0 + p_1 \left[p_0 + p_1 z + p_2 z^2\right] + p_2 \left[p_0 + p_1 z + p_2 z^2\right]^2\\
   = &  p_0 + p_1 p_0 + p_1^2 z + p_1 p_2 z^2 + p_2p_0^2 + (2p_0 p_1 p_2)z + (2p_0 p_2^2 + p_2p_1^2)z^2 + (2p_1 p_2^2)z^3 + p_2^3 z^4
  \end{aligned}$$

## Extinction

- Extinction
  - "Extinction is forever.": So, the probability $d_n$ of extinction _by_ generation $n$ can never decline over time. (Must it
    always rise?)

- Recursive extinction
  - Is non-extinction "forever"?: If $\lim_{n \rightarrow \infty} = d(\infty) < 1$, then this says there's a chance $1 - d(\infty)$ of eternal persistence. We'll try to figure out more about what this means.
  
- If the probability of a female line going extinct in $n$ generations is $d_n$, then this is equivalent to her daughter(s) line(s) going extinct in $n-1$ generations. With $p_k$ chance of having $k$ daughters, we have
  $$
  d_n = p_0 + p_1 d_{n-1} + \mbox{What is next term in series?}
  $$

  - What can we do with 
  $$
  d_n = h(d_{n-1})?
  $$ 
  
  - Well, remember that $d_n$ is non-decreasing, and that it's maximum can be no greater than $1.0$. When $d_n$ reaches it's limit, say $d$,  we won't need generational subscripts, $d$ will be constant, and will obey
  $$
  d = h(d)
  $$

  - Thus, an amazing result: the probability of ultimate extinction is when the argument equals the PGF of the argument.
- Can $d = 1$, can $d < 1$
  - Try $d = 1$. What happens?
  - If we were to find a solution less than 1.0, how would we
    interpret that?
  - Three cases
```{r}

  z = seq(0, 1.6, .01)
  pk = c(.3, .0, .7); names(pk) <- 0:2
  d <- pk["0"]
  for (i in 1:10)
  {
      d <- pk["0"]  + pk["1"]*d + pk["2"]*d^2
  }
  ##  super-critical 
  hz = pk["0"]  + pk["1"]*z + pk["2"]*z^2
  plot.fun <- function(z, hz)
  {
      plot(z, hz, type = "l", ylim = c(0,1.6),
           ylab = "h(z)",
           yaxs = "i", xaxs = "i", axes = F)
      axis(1, at = seq(0, 1.5, .5))
      axis(2, at = seq(0, 1.5, .5))
      abline(0,1, col = "grey")
      lines(z, hz)
      axis(2, at = pk["0"], labels = "p0",
           col.axis = "red", col = "red",
           lwd = 1, las = 2)
  }
  
  par(mfrow = c(1,3), pty = "s")
  plot.fun(z,hz)
  points(c(d, 1),c(d, 1))
  title("Super-critical (m > 1) \n 2 roots")
  ## sub-critical
  pk = c(.3, .55, .15); names(pk) <- 0:2
  hz = pk["0"]  + pk["1"]*z + pk["2"]*z^2
  plot.fun(z,hz)
  title("Sub-critical (m < 1) \n 1 root")
  points(1,1)
  ## critical
  pk = c(.3, .4, .3); names(pk) <- 0:2
  hz = pk["0"]  + pk["1"]*z + pk["2"]*z^2
  plot.fun(z, hz)
  title("Critical (m = 1), \n 1 root")
  points(1,1)
```

  - We can prove by answering: What is $h'(1)$? What is $h(0)$? Is $h''(z) > 0$? 

  - In the following cobwed diagram (like a staircase), we can see the values of $h(p_0)$, $h(h(p_0))$, $h(h(h(p_0)))$, $\ldots$?
    - The orange dot is $h(p_0)$.
    - The purple dot is $h(h(p_0))$.
    - The blue dot is $h(h(h(p_0)))$.
    - The points would converge until we reach the intersection between $h(z)$ and the diagonal.
```{r,  echo= FALSE}
  pk = c(.3, .0, .7); names(pk) <- 0:2
  z = seq(0, 1.6, .01)
  hz = pk["0"]  + pk["1"]*z + pk["2"]*z^2
  plot.fun(z,hz)
  hzp0 <- pk["0"]  + pk["1"]*pk["0"] + pk["2"]*pk["0"]^2
  hhzp0 <- pk["0"]  + pk["1"]*hzp0 + pk["2"]*hzp0^2
  hhhzp0 <- pk["0"]  + pk["1"]*hhzp0 + pk["2"]*hhzp0^2

  segments(x0=0, y0=pk["0"],x1=pk["0"], y1= pk["0"], col = "orange", lwd=2)#h(p_0)
  segments(x0=pk["0"], y0=pk["0"], y1= hzp0, col = "orange", lwd=2)
  
  segments(x0=pk["0"], y0=hzp0,x1=hzp0, y1= hzp0, col = "purple", lwd=2) #h(h(p_0))
  segments(x0=hzp0, y0=hzp0, y1= hhzp0, col = "purple", lwd=2)

  segments(x0=hzp0, y0=hhzp0,x1=hhzp0, y1= hhzp0, col = "blue", lwd=2)
  segments(x0=hhzp0, y0=hhzp0, y1= hhhzp0, col = "blue", lwd=2)
  
  points(x= pk["0"], y =hzp0, pch=19, col="orange") #h(p_0)
  points(x= hzp0, y =hhzp0, pch=19, col="purple") #h(h(p_0))
  points(x= hhzp0, y =hhhzp0, pch=19, col="blue") #h(h(h(p_0)))


``` 
- So how do we actually get $d$?
  - Take the case where $p_0 = .3$, $p_1 = 0$, and $p_3 = .7$ (the one I just plotted).
    - Can do some algebra
    - Or we can recursively iterate on the computer.
      - Numerical recursion: below we start at the first value of the probability vector and sequentially try exploring values of $h(z)$ until we find one that doesn't change as much. We illustrate the case for up to 20 iterations.  
```{r}
  pk = c(.3, .0, .7); names(pk) <- 0:2 ## our example
  d <- pk["0"] # initial value
  for (i in 1:20)
  {
      d <- pk["0"]  + pk["1"]*d + pk["2"]*d^2
      if (i %in% c(1,2,19,20))
          print(paste(i, d))
  }
```
      - Did we get the right value? Apparently, yes! The green lines take as vertical and horizontal values our $d=0.428568100698915$  
```{r, echo=FALSE}
  pk = c(.3, .0, .7); names(pk) <- 0:2
  z = seq(0, 1.6, .01)
  hz = pk["0"]  + pk["1"]*z + pk["2"]*z^2
  plot.fun(z,hz)
  abline(h = d, col = "green")
  abline(v = d, col = "green")
```

- Extinction and non-extinction revisited  
  - If $m > 1$, there exists $d$ bigger than 0 and less than unity.
  - This means there's some positive chance of extinction.
  - But also some chance of never-extinction. (What form does never-extinction take?)

```{r, echo=FALSE, fig.cap='Generation sizes (log-scale) over many generations'}
    suppressWarnings(matplot(t(Z.mat), log = "y", type = "l", lty = 1, ylab = "Zn", xlab = "n"))
```

Relevance to Corona virus?


## Good and bad set-ups for branching process


<style>
.column-left{
  float: left;
  width: 50%;
  text-align: left;
}
.column-right{
  float: right;
  width: 50%;
  text-align: left;
}
</style>


<div class="column-left">
**Good**

- Unrestricted growth (frontier, new disease, start of a reaction)

- A "null" model for understanding how apparent structure
is just random outcomes. Families that die out didn't have to
have low $NRR$. Just because most new viruses don't break out,
doesn't mean they aren't potentially dangerous ($R_0 >> 1.0$).

- A model that corresponds our mental model of running a
          generative process forward. (cf. Fisher-Wright)
</div>
<div class="column-right">
**Bad**

- When offspring of 1 depends on offspring of other  (e.g., brothers inheriting a farm)

- When resource constraints slow growth rates (e.g.,
Malthus: fertility of  next gen depends on fertility of last;
SIR model in disease spread)

- Analysis. PGF is powerful but still we often have to deal
          with listing all possibilities.

- Big populations -- law of large numbers means randomness
          doesn't matter.
</div>



## The distribution of offspring of all generations

- Means of offspring in generation $n$  
  - Is it "meaningful"? It tells you about the average number of offspring but it might be a skewed distributions because of the large number of zeros.
  - We'll show that unconditional mean is the expectation of  random sum, which in this case is the expectation of a product.
    $$
    \mathbb{E} Z_n = m^n
    $$
  - What if $m = 1$?
  
- Mean size of surviving lines?  
    - Total probability is sum of conditional probabilities, times the chance of each condition:
  $$
  \mathbb{E} Z_n = \mathbb{E}( Z_n | Z_n > 0) P(Z_n > 0) +
  \mathbb{E}( Z_n | Z_n = 0) P(Z_n = 0)
  $$
    - where $\mathbb{E}( Z_n | Z_n > 0) P(Z_n > 0)$ is the expected size given no extinction and $\mathbb{E}( Z_n | Z_n = 0) P(Z_n = 0)$ is the expected size given extinction.  
    - What is mean size of surviving lines? That is, we want to find $\mathbb{E}( Z_n | Z_n > 0)$
      - Hint 1: $P(Z_n = 0) = d_n$
        - So $P(Z_n > 0) = 1- d_n$
      - Hint 2: $\mathbb{E} Z_n = m^n$
      - Also note that by definition, $\mathbb{E}( Z_n | Z_n = 0)=0$
      $$
      \begin{aligned}
      \mathbb{E} Z_n = m^n = & \mathbb{E}( Z_n | Z_n > 0) (1- d_n) + (0)d_n\\
      m^n = & \mathbb{E}( Z_n | Z_n > 0) (1- d_n)\\
      \mathbb{E}( Z_n | Z_n > 0) =&\frac{m^n}{(1- d_n)}
      \end{aligned}
      $$
      
    - Let's check our result using simulation for 1000 trials. Note that for this case $m= 0\times p_0 + 1\times p_1  + 2\times p_2= 1$, so $m^n= 1$  
    
```{r}
  n_trials = 1000; n_gen = 100
  p0 = .3; p1 = .4; p2 = .3 ## what is m?
  p_k <- c(p0, p1, p2)
  k <- 0:(length(p_k)-1)
  Z.mat <- matrix(NA, n_trials, n_gen)
  set.seed(131)
  for (i in 1:n_trials)
      Z.mat[i,] <- branch(n_max = n_gen)
```
```{r, echo=FALSE}
Zn_bar = apply(Z.mat, 2, mean)
  matplot(t(Z.mat),
          type = "l", lty = 1, ylab = "Zn", xlab = "n")
```
    - Using the means for all 1000 simulations, we calculate $\mathbb{E} Z_n$ (left plot), $d_n$ (center plot) and $\mathbb{E}( Z_n | Z_n > 0)$.   
```{r, fig.height = 5}
Zn_bar = apply(Z.mat, 2, mean)  #expected value Z_n (E(Z_n))
n <- 1:ncol(Z.mat)
proportion.zero <- function(x){prop.table(table(x == 0))["TRUE"]}
d_n = apply(Z.mat, 2, proportion.zero) # fraction extinct 
Z.mat.na <- Z.mat; Z.mat.na[Z.mat == 0] <- NA 
Zn_surv_bar = apply(Z.mat.na, 2, mean, na.rm = T) #expected value of survivors (E(Z_n|Z_n>0))
```
```{r, echo=FALSE}
par(mfrow = c(1,3))
plot(n, Zn_bar, main = "Mean Zn")
plot(n, d_n, main = "Fraction extinct")
plot(n, Zn_surv_bar, main= "Mean Zn given Zn>0") 
## insert code here for Zn_surv_bar.hat and add a line
```
 
  - Proving  $\mathbb{E} Z_n = m^n$ 
    - Ingredients  
      - $h(z)=p_{0}+p_{1}z+ p_{2} z^2+ \ldots$
      - $h'(z)=p_{1}+ 2zp_{2} + \ldots$      
      - Specifically, when $z=1$:
        - $h(1)= 1$
        - $h'(1) =(1)p_1+ (2)p_2+(3)p_3+\ldots= \sum_1^k k p_k = m = \bar{Z}$
        - $h_n'(1) = m_n = \bar{Z}_n$
      - $h_{n+1}(z) = h(h_n(z))$
    - Derivation: 
      Take derivative of $h_{n+1}(z)$
      $$
      \begin{aligned}
      h'_{n+1}(z)&= h'_n(h(z)) h'(z) \\
      h'_{n+1}(1)&= h'_n(h(1)) h'(1) \\
      &= h'_n(1) m \\
      &= \bar{Z}_n m
      \end{aligned}
      $$ **NOTE: Can't find the complete derivation. Help!**
      

  - Variance result.  
    - We proved that $\mathbb{E} Z_n = m^n$ using recursion. For the variance, one can also do recursion but this time we start by taking the second derivative of $h(z)$ 
    - For $m = 1$,
  $$
  \sigma_n^2 = n \sigma^2 
  $$

  - Also a result for $m\neq 1$
  - What does increasing variance mean for critical case? (Does
    this make sense?)
  - What happens to variance of lines that survive? Is it bigger
    or smaller than unconditional variance?

Variance in our simulation increases with generation number: 
```{r}
var_Zn = apply(Z.mat, 2, var)
```
```{r, echo=FALSE}
n <- 1:ncol(Z.mat)
plot(n, var_Zn, main="Variance of Zn")
```
Distribution of $Z_n$
```{r, fig.height = 6}
   Z20 <- table(table(Z.mat[,20]))
   Z5 <- table(table(Z.mat[,5]))
   par(mfrow = c(2,2))
   plot(Z20[Z20 < 100])
   plot(log(Z20[Z20 < 100]))
   plot(Z5[Z5 < 100])
   plot(log(Z5[Z5 < 100]))
```


## Geometric offspring distribution

  For $k = 1, 2, \ldots$,
  $$
  p_k = b c^{k-1}
  $$

  For $k = 0$,
  $$
  p_0 = 1 - p_1 - p_2 - \ldots .
  $$

  Let's solve for $p_0$, using the geometric series sum, for $a <
  1$,  
  $$
  1 + a + a^2 + \ldots =
  1 / (1-a)
  $$
$$
\begin{aligned}
p_0 &= 1  - (p_1 + p_2 + p_3 +\ldots) \\
&= 1 - (bc^{0}+ bc^{1} +b c^{2} + \ldots) \\
&= 1 - (b+ bc^{1} +b c^{2} + \ldots) \\
&= 1 - b(1+ c^{1} + c^{2} + \ldots) \\
&= 1-  \frac{b}{1-c} \\
\end{aligned}
$$
So, now we have all $p_k$ used as inputs for $h(z)$ given value for $b$ and $c$.

A picture, Lotka's parameters for 1920. The graph shows the probability of having exactly $k$ number of girls. When $k=0$ then all children were boys.
```{r}
b = 0.2126 ;   c = 0.5893
kk = 1:10  ;   p_kk = b * c^(kk-1)
p0 = b/(1-c)
k = c(0, kk) ;   p_k = c(p0, p_kk)
```
```{r, echo=FALSE}
plot(k, p_k)
```

Realism? See Table 10.3, p 386, Grinstead and Snell.



### The Geometric Distribution's simple PGF

  
  $$
  h(z) = p_0 + p_1 z + p_2 z^2 + \ldots
  $$
  With geometric $p_k$
  $$
  h(z) = p_0 + \sum_{k= 1}^\infty b c^{k-1} z^k.
  $$
  Substituting for $p_0$ and rewriting
  $$
  h(z) = \left( 1 - \frac{b}{(1-c)}\right) + bz \sum_{k= 1}^\infty 
  (cz)^{k-1}.
  $$
  Substituting $j = k-1$,
  $$
  h(z) = \left( 1 - \frac{b}{(1-c)}\right) + bz \sum_{j= 0}^\infty 
  (cz)^{j} = \left( 1 - \frac{b}{(1-c)}\right) + {bz \over (1 - cz)}
  $$

  The PGF is now "tractable"
  
- $m$ and extinction
  $$
  h(z) =  \left( 1 - {b / 1-c}\right) + {bz \over 1 - cz}
  $$
    - Please solve for $m$. (Hint: $h'(1)$). 
      - We kmow that $h'(1)=m$, so we start off taking the first derivative of $h(z)$ given the tractable form.
      $$
      \begin{aligned}
      h'(z) &= \frac{bz(-c)-(1-cz)b}{(1-cz)^2}\\
      &= \frac{-1}{(1-cz)^2}\\
      h'(1) &= \frac{-1}{(1-c)^2} = m\\
      \end{aligned}
      $$
    - What is $m$ with Lotka's $b$ and $c$? 
    $$
    m= \frac{-1}{(1-0.5893)^2}= 5.928579
    $$
    **NOTE: does this look reasonable?**
  
- We solve $z = h(z)$ with a bunch of algebra to get
  $$
  d = {1 - b - c \over c(1-c)}
  $$
    - How does $d$ depend on $b$ and $c$? $d$ decreases with $b$  at a rate of $\frac{-1}{c(1-c)}$ and changes at rate $\frac{-(1-b)(1-2c)-c^2}{c^2(1-c)^2}$.

Big payoff: the full distribution of $Z_n$}
  See Grinstead and Snell p. 385
  
### A plot of Keyfitz's numbers for generations 1, 2, and 3. Is it exponential for $k > 0$?
```{r}
## b = 0.2126 ;   c = 0.5893 ## lotka
b = 0.3666; c = .5533 ## Keyfitz (from GS)
m = b / (1-c)^2 ## [1] 1.260416
d = (1 - b - c) / (c * (1-c))           #[1] 0.8185088

par(mfrow = c(1,2))
for (i in 1:3)
{
    n = i
    p0_n = d * (m^n - 1)/ (m^n -d)
    j = kk
    pj_n = m^n *
        ((1-d) / (m^n - d))^2 *
        ((m^n - 1)/(m^n - d))^(j-1)
    pk_n <- c(p0_n, pj_n)
    if (i == 1)
        plot(k, pk_n, type = "l", log = "")
    if (i > 1)
        lines(k, pk_n, col = i)
}

for (i in 1:3)
{
    n = i
    p0_n = d * (m^n - 1)/ (m^n -d)
    j = kk
    pj_n = m^n * ((1-d) / (m^n - d))^2 * ((m^n - 1)/(m^n - d))^(j-1)
    pk_n <- c(p0_n, pj_n)
    if (i == 1)
        plot(k, pk_n, type = "l", log = "y", main="Log scaled")
    if (i > 1)
        lines(k, pk_n, col = i)
}
```

- Applications

  We have exponential distribution with a few  very large lines, and a lot of small lines.
  
  - Distribution of neutral alleles
  - Distribution of family lines (Y-chromosome, mtDNA, last names)
- Our result
  With geometric $p_k$, we get geometric $Z_n$, for all $n$.

  Conjecture: geometric is to BP as gamma is to frailty? 



## Branching Processes and Covid-19

  - What is the BP that they are studying? Is it contagion, social
    contacts, or ?
  - What do they assume about the BP?
  - Do they use any analytical results or just simulation? Why?
  - Best feature of paper?
  - Worst feature of paper?
  - Inspire any other approaches?

<!--chapter:end:07-branching.Rmd-->

# Fisher-Wright

## Outline
- Fisher Wright vs Galton Branching Process
- FW with mutation
- Extinction
- Application: Baby Names

Additional resources:

  - Blog "Introduction to the Wright-Fisher Model" <https://stephens999.github.io/fiveMinuteStats/wright_fisher_model.html#pre-requisites>
  - @hahn2003drift:  Evolutionary anthropologists arguing that the neutral explanation of the Fisher-Wright model is consistent with the distribution of 1st names. What other quantitative or qualitative features of 1st name fashion could be used to try to reject the neutral model?
  - @felsenstein2005theoretical: Very complete "lecture notes" for graduate genetics course. Lots of good commentary, does not assume a lot of math background, but lots of content and can be diffcult to read a piece by itself. 

## Parallel


<style>
.column-left{
  float: left;
  width: 50%;
  text-align: left;
}
.column-right{
  float: right;
  width: 50%;
  text-align: left;
}
</style>


<div class="column-left">
*Fisher-Wright*

  - Children picking their parents (not "generative")
  - Total population size is constant
  - Qualitatively similar to BP. Extinction and fixation.
  - Flexible: mutation, selection, even changes in pop size.
  - With apologies, biologists take FW "seriously" even if they
    don't take it "literally".
    
</div>
<div class="column-right">
  *Galton-Watson-Bienaym\'e Branching Processe*
  
  - Branching process models independent parents randomly
    producing offspring. "Generative"
  - Total population size can vary, and has a random component and
    deterministic one $m$
  - Qualitative result when $m = 1$ is that there is one longest
    surviving line. This is "fixation", when one _type_ becomes
    universal.
  -
    
</div>

### Another cell phone example
Let's simulate and draw lines

Gen 1   0    1    2    3    4    5    6    7    8    9

Gen 2

Gen 3

- What will happen?

## Mutation

Simulation

```{r}
  fwm <- function(N, n_gen, mu = 0) ## mu != 4/N
  {
      ## simulate fisher-wright (with mutations)
      x <- paste(1:N) ## starting types
      A <- matrix(NA, nrow = n_gen, ncol = N)
      for (i in 1:n_gen)
      {
          A[i,] <- x
          x <- sample(x, size = N, replace = T)
          x <- mut(x, mu)
          x
      }
      return(A) ## matrix of types, each line a generation.
  }
```


```{r}
  mut <- function(x, mu)
  {
      ## m, the individuals that mutate
      m <- which(rbinom(length(x), 1, mu) == 1)
      if (length(m) == 0) ## if no-one mutates
          return(x)
      ## add a suffix to their ID, so it will be unique (infinite alleles)
      suffix <- 10000*round(runif(length(m)),4)
      x[m] <- paste0(x[m], ".", suffix)
      x
  }
```

Trying it out
```{r}
  set.seed(1)
  fwm(N = 10, n_gen = 2, mu = 0)
  fwm(N = 10, n_gen = 2, mu = 0)
```   

Trying it out some more

```{r, fig.height = 3.5}
  set.seed(1)
  A <- fwm(N = 10, n_gen = 20, mu = 0)
  tt <- table(A, row(A)) ## count types by row
  ptt <- prop.table(tt, 2) ## proportions
  matplot(t(ptt), type = 'l', lty = 1, main = "FW simu")
  text(x = 4, y = jitter(ptt[,4]), rownames(ptt), col = 1:6)
``` 

Questions: 

- What happens at time 15?
- Why does line 5 rise and fall?
- What happens at time 2?
- What is $E(p_i(t) | p_i(t-1))$?


Bigger pop and more time
```{r, fig.height = 4}
  set.seed(1)
  A <- fwm(N = 100, n_gen = 200, mu = 0)
  tt <- table(A, row(A)) ## count types by row
  ptt <- prop.table(tt, 2) ## proportions
  matplot(t(ptt), type = 'l', lty = 1)
``` 
  
  - What does this remind you of? What will happen in long run?
  - What other questions could we ask?
  
## Fixation
Questions we can ask
  
  - What is probability that line $i$ will ``fix''? (Hint: easy)
  - What is expected time until some line fixes? (We'll demo the result)
  - How can we describe the path to fixation? (We'll derive the result)
  
  
Probability that a particular line will "fix"
```{r, fig.height = 5}
  set.seed(1)
  A <- fwm(N = 10, n_gen = 20, mu = 0)
  tt <- table(A, row(A)) ## count types by row
  ptt <- prop.table(tt, 2) ## proportions
  matplot(t(ptt), type = 'l', lty = 1, main = "FW simu")
  text(x = 4, y = jitter(ptt[,4]), rownames(ptt), col = 1:6)
``` 

Expected time until fixation?

  Answer for us is
  $$
  \bar{T}_{fixed} = 2 \cdot N
  $$
  Note: Biologists say $4 N_e$. See Wikipedia "Genetic drift"
  

Simulation of time to fixation
```{r}
  T.vec <- NULL
  all.the.same <- function(x){length(unique(x)) == 1}
  set.seed(10)
  for (i in 1:100)
  {
      A <- fwm(N = 100, n_gen = 1000,mu = 0)
      extinction_time = min(row(A)[apply(A, 1, all.the.same)])
      T.vec[i] <- extinction_time
  }
  mean(T.vec)
```

### Path to fixation: a measure of homogeneity/heterogeneity

  Chance that two randomly drawn individuals are of same type.
  $$
  G = \sum_i p_i^2
  $$

  If we have two types, $p_1 = \pi$ , $p_2 = 1-\pi$?
  What is G if $\pi = 0, .5, 1$?


Let's derive time path of G

  Let's assume just two types, $\pi(t)$

  Chance two indiv are of same type
  $$
  G_{t+1} = P(\mbox{same parent})\cdot 1 +
  P(\mbox{different parent})\cdot G_{t}
  $$

  Notation: I'm going use $K$ for pop size. Bio uses $2N$.
  $$
  G_{t+1} = {1 \over K} \cdot 1 + (1 - {1\over K}) \cdot G_{t}
  $$

  Easier to solve letting $H = 1 - G$. Some algebra gives
  $$
  H_{t+1} = H_{t} (1 - 1/K)
  $$

  Or,
  $$
  H_{t} = H_0 (1 - 1/K)^t % \rightarrow H_0 e^{-t/K}
  $$
  So, H goes to 0 exponentially, just as G goes to 1.


## Baby Names

>  "Drift as a mechanism for cultural change: an example from baby names"
by Matthew W. Hahn and R. Alexander Bentley Proc. R. Soc. Lond. B 2003 270, S120-S123

### What's the basic idea?
  
  - How is naming a baby like Fisher-Wright?
  - How is it not?
  
Applying

  - Like Fisher-Wright 
    - people choose from existing set (?)
    - names are "neutral" (?)
    - draw proportionally (?)
  - They test to see if they can reject FW
    - compare observed histograms to FW simulation
  - They include mutation to get a stationary disn
  Note: failing to reject FW doesn't mean it's correct

Their picture 

![Baby Names by Matthew W. Hahn and R. Alexander Bentley Proc. R.Soc. Lond. B 2003 270, S120-S123](/hdir/0/fmenares/Book/bookdown-master/images/hahn_pic.png)


### Fisher-Wright simulation of Baby Names (Hahn and Bentley)

Drwaing their Picture

Data prep

```{r}
download.file(url= "https://www.ssa.gov/oact/babynames/names.zip",
              "./names.zip")
unzip("names.zip", exdir = "./names")

library(data.table)

filenames <- system("ls ./names/*.txt", intern = T)
mylist <- vector("list", length(filenames))
names(mylist) <- gsub(pattern = "[^0-9]", replace = "", filenames)
for (i in 1:length(filenames))
{
    myfile <- filenames[i]
    mylist[[i]] <- fread(myfile)
}

dt <- rbindlist(mylist, idcol = "year")
names(dt) <- c("year", "name", "sex", "N")
``` 

ok, we have the data now


Plot observed frequencies
```{r}
## male 1900-1909
my.dt <- dt[sex == "M" & year %in% 1900:1909]
foo <- my.dt[, .(N = sum(N)), by = name]
foo <- foo[order(N, decreasing = T)]
bar <- foo[1:1000,] ## 1000 top names
```

now let's do a power law plot
```{r}
my.breaks <- c(0, 2^(0:11)/10000)
bar[, p := round(prop.table(N),5)]
bar[, pcat := cut(p, breaks =  my.breaks, right = F, include.lowest = T)]

out <- unclass(prop.table(table(bar$pcat)))


my.x <- my.breaks[-length(my.breaks)] + diff(my.breaks)/2
plot(my.x, out, log = "xy")
```

### Drawing their picture with simulation

FW simulation
```{r}
mut <- function(x, mu)
{
    ## m, the individuals that mutate
    m <- which(rbinom(length(x), 1, mu) == 1)
    if (length(m) == 0)
        return(x)
    suffix <- 10000*round(runif(length(m)),4)
    x[m] <- paste0(x[m], ".", suffix)
    x
}

fwm <- function(N, n_gen, mu = 0)
{
x <- paste(1:N)
A <- matrix(NA, nrow = n_gen, ncol = N)
for (i in 1:n_gen)
{
    A[i,] <- x
    x <- sample(x, size = N, replace = T)
    x <- mut(x, mu)
    x
}
return(A)
}
```

Let's look at evolution over time of G: chance that two individuals are of same type
```{r}
get.G <- function(x)
{
    tt <- table(x)
    p <- prop.table(tt)
    sum(p^2)
}
```

without mutation
```{r}
A <- fwm(1000, n_gen = 4000, mu = 0)
G.vec <- apply(A, 1, get.G)
plot(G.vec)
```

with mutation, 1 trial

```{r}
N = 1000
A <- fwm(N, n_gen = 3000, mu = 4/N)
G.vec <- apply(A, 1, get.G)
plot(G.vec)
```
average over 100 trials

```{r}
n_gen = 2000
n_trials = 100
G.mat <- matrix(NA,  n_trials, n_gen)
for (i in 1:n_trials)
    {
        N = 1000
        A <- fwm(N, n_gen, mu = 4/N)
        G.vec <- apply(A, 1, get.G)
        G.mat[i,] <- G.vec
        }
matplot(t(G.mat), type = "l")

G.bar <- apply(G.mat, 2, mean)
lines(G.bar, lwd = 4)
abline(h = 1/9, lty = 3, col = "yellow", lwd = 5)
```
cool plot, why is it about .11?
```{r}
1/(1 + 8)

```

Gillespie tells us that Gbar is supposed to be 1 / (1 + 4*Ne*mu)

How does 4*Ne*mu = 8?

Well, we have $K*mu = 4$ and since $K = 2*Ne$, $Ne = = K/2$ (maybe)

### FW babyname simulation of equilibrium frequencies 

```{r}
N = 1000 ## Not sure if this is (w)right :)
mu = 4/N ## [1] 0.004
theta = N*mu ## [1] 4 ## H&B's "best fit"
```

## Now we can simulate babyynmes
```{r}
n_gen = 1001
N = 1000
## set.seed(1)
## A <- fwm2(N, n_gen, mu = 4/N)
#A <- fwm2(N, n_gen, mu = 8/N)
###############
#What about the fwm2 function?
#######################
A <- fwm(N, n_gen, mu = 8/N)
## ok, lets do power law plot of this
x <- A[1001,]
tt <- table(x)
ptt <- prop.table(tt)
my.breaks <- c(0, 2^(0:11)/10000)
p <- ptt
## bar[, p := round(prop.table(N),5)]
##bar[, pcat := cut(p, breaks =  my.breaks, right = F, include.lowest = T)]
pcat = cut(p, breaks =  my.breaks, right = F, include.lowest = T)
out <- unclass(prop.table(table(bar$pcat)))
out.hat <- unclass(prop.table(table(pcat)))
my.x <- my.breaks[-length(my.breaks)] + diff(my.breaks)/2
plot(my.x, out, log = "xy")
lines(my.x, out.hat)
```

## Conclusions
  
  - Fisher-Wright an alternative to branching processes
  - It reverses logic of reproduction, but gives similar
    quantitative and qualitative results
  - A neutral model for other processes?
  - Starting point for coalescent

### Some potential criticism
  
  - While we can't reject that there's some parameterization of
    FW that gives us similar disn, this doesn't mean that we've
    found the right mechanism. (Just that we can't reject it).
  
  - What are some other tests of this mechanism?
  
  - Markov assumption. We could see if each frequency really
    followed random walk.
  - Perhaps we could see if variances were scaled to
    frequencies correctly.
  

<!--chapter:end:08-fw.Rmd-->

# Coalescent 


## Outline   
  
  - Big picture: What is "coalescent theory"?
  - Time to (T)MRCA
  - Simulation: Inferring population size
  - An application of Coalescent Theory

## Big picture: The Coalescent: Expectations of the Past
  
  - Coalescent theory is not a theory.
  - It's a model for the probability of different histories
  - "the" coalescent is a bit confusing. We're not inferring the
    actual history of common ancestry, just the probabilities
 
An actual "picture"
![](/hdir/0/fmenares/Book/bookdown-master/images/cutter_diagram.png)


- Top panel is a Fisher-Wright instance, ordered so that lines
         don't cross.
- Haplotype is a sequence (we are diploids, each 
         contributing 2 haplotypes). But let's just think of each line as
         an individual, for now.
- We can find The Most Recent Common Ancestor (TMRCA) of 
sample (dark purple). Who and when would the MRCA of the top two individuals be?


  - Our sample $\neq$ even all _extant_ descendants of the MRCA. What
    does this mean?
  - Our sample $\neq$ all of the descendants of the MRCA. What
    does this mean?
  - If we chose two descendants at random, would we always get
    same MRCA? 
  
When we model coalescence we are thinking backwards in time.
  
## Our first question: When was MRCA?

  If we sample two individuals (today), how long ago was their MRCA?\\
  (Note: question is not "who")
  
  - Our answer will in terms of the probability of MRCA being 1
    generation ago, 2 generations ago, etc.
  - We'll assume Fisher-Wright (constant N, each gen randomly
    picks parents)
  - The answer is surprisingly simple
  
Let's assume we have $N$ lines in Fisher-Wright (Note: I'm not using $2N$.)
  
  - The chance that two sampled people have same parent is $1/N$, right?
  - Thus $P(T_{MRCA} = 1) = 1/N$.
  - What is $P(T_{MRCA} = 2) = $?
  - What is $P(T_{MRCA} = n) = $?
  
  Let's go to continuous time (reasonable if pop is big and time scale
  is long).
  
  Hazard of coalescence = $c = 1/N$.
  Probability of coalescence at time $t$ = $\ell(t) h(t) = e^{-ct} c$

  What is expected time of coalescence? Think life expectancy.
  
  $E(T_{MRCA})$ if two samples: $1/c = 1/(1/N) = N$

  Let's simulate

  - 1 time, without random seed, letting N = 40, ngen = 200, mu = 0
  - Average over 100 FW simulations
  - What is variance of outcome? Is it what we would expect from
    exponential?


## Mutation and inference of TMRCA and $N$
  
  - Say mutations occur at a constant rate $\mu$ ($10^{-8}$?)
  - Each year we would expect $\mu$ mutations, and over $T$ years we
    would expect $T\mu$ mutations.
  - Say we observe that two people differ at $k$ sites of the
    genome.
    - When was TMRCA?
    - How big is the population?
  


  Picture ($\Lambda$)
  Tree length = $2T$
  Expected number of mutations:
  $$
  E(k) = E(2T\mu) = \bar{T} 2 \mu
  $$
  Since,
  $$
  \bar{T} = E(TMRCA) = N
  $$
  If we observe on average $\bar{k}$ mutations, then
  $$
  E(k) = N 2 \mu \rightarrow \hat{N} = {\bar{k} \over 2 \mu}
  $$ 

## Inference of population size, simulation
  
  - We do FW with mutations
  - Average pairwise differences
  - Divide by $2\mu$ to get our estimate
  - We can repeat a bunch of times and see average estimate
    converges to the truth
  

Coalescence of a sample of $n$ individuals
  
  - This is covered on pages 42 and 43 of Gillespie
  - We'll just do one quick example, accepting the result
  
A sample of 3: Note we're using $N$ (instead of $2N$)

\begin{verbatim}

        *            _______
       / \
      /   \          
     /     \         T(2) : E(T(2)) = N
    /       \
   /         *       _______
  /         / \
 /         /   \     T(3) : E(T(3)) = N * 2/[3*2]
*         *     *    _______

                      ...
                     _______
                     T(n) : E(T(n)) = N * 2/[n * (n-1)]
\end{verbatim}
  Question: If we sample 4, how much of time to TMRCA
  is do we have 4 branches, 3 branches, and 2 branches?

## Conclusions


  - We defined the coalescent as the stochastic process going
    back in time to common ancestors
  - For constant population size, we proved that time to
    coalescence for a sample pair is exponential.
  - We showed (math and simulation) that $E(T) = N$.
  - We showed that we could estimate $N$ from observed mutations
    if we knew the mutation rate


\end{frame}

## An application of coalescent theory
  
Application to making inferences about the real history of human populations.
    

 
Caveat: We're still using $N$ (not $2N$) as the number of haploids

### Coalesence when population is changing
  
  - Ee said hazard of coalescence was $h = c = 1/N$.
  - What is hazard of coalesence in one generation for two
    different populations: $N =
    1000$? $N = 2000$?
    
  - What if within the same population $N(t) = 1000$ and $N(t+1)
    = 2000$? (Hint: we still follow FW in allowing children to
    choose their parents.)
    
  - If the population size changes over time $N(t)$, then hazards
    of coalescence in will change too: $h(t) = 1/N(t)$.
  
```{r ,fig.height = 3}
  N_recent = 5000 ## population last T_thresh years
  T_thresh = 1000
  N_ancient = 500 ## earlier population
  n = 1000 ## sampled individuals
  set.seed(0.4886)
  T1 <- rexp(n, rate = 1/N_recent) ## give everyone a chance to coalesce 
  T1[T1 > T_thresh] <- NA ## if they don't in 1st 1000 years, resample them
  n2 <- sum(is.na(T1))
  T2 <- T_thresh + rexp(n2, rate = 1/N_ancient) ## at ancient rate
  T.vec <- c(T1, T2)
  hist(T.vec, breaks = seq(0, 5000, 250))
```
  Q: How could we estimate population sizes from this histogram?
  
```{r, fig.height = 3}
  T.vec <- sort(T.vec)
  St = (n:1)/n
  par(mfrow = c(1,2))
  plot(T.vec, St); abline(v = T_thresh); plot(T.vec, log(St)); abline(v = T_thresh)
```
  Q: How can we estimate hazards from this histogram?



Our approach
  
  - Say we have $i$ pairs of haploids
  - We then compute how many pairwise differences there are,
    but instead of computing $\bar{k}$, we keep the distributional
    information $k_i$.
  - Each $k_i$ implies a $T_i$
  - We then have a set of ``death times'' (coalescence times),
    can build a life table, estimate the hazards, and infer
    $N(t)$.

## Inference from MRCA times: $n > 2$ comparisons and relative branch lengths
    
Detour to length of branches
A sample of 3: Note we're using $N$ (not $2N$)
\begin{verbatim}

        *            _______
       / \
      /   \          
     /     \         T(2) : E(T(2)) = N
    /       \
   /         *       _______
  /         / \
 /         /   \     T(3) : E(T(3)) = N * 2/[3*2]
*         *     *    _______

                      ...
                     _______
                     T(n) : E(T(n)) = N * 2/[n * (n-1)]
\end{verbatim}

  Intuition: when we have more individuals, there's more chance that
  some pair of them will coalesce.
  
  Question: If we sample 4, how much of time to TMRCA
   do we have 4 branches, 3 branches, and 2 branches?


## Reconstruction Ancient European Population Sizes using Batini's sample of Mitochondrial DNA"

We use real sequences of mitochondrial DNA to estimate
ancient population sizes. The sequences were made available by Batini
et al, who analyzed sub-populations in their paper using software for
Bayesian inference for comparing groups of individuals. Our approach
here is to use a simpler, but less powerful approach. We will simply
look at pairwise differences of random pairs of individuals. For this
we are not going to do subgroups such as the Greeks or Irish, but are
going to use the entire European sample. The population sizes we
estimate are for the entire population represented by the 328
individuals. 

>Special thanks to Ken Wachter, who taught me this approach and whose R-code forms the basis of this application

### Summary of Batini et al.

Before we begin our own analysis, let's look at the inputs to
Batini's analysis -- the mtDNA haplotype sequences, and then at the
resulting population estimates.

#### The mitochondrial DNA

Data preparation begins. 

There are 380 individuals grouped into regional sub-populations. I select out the Greeks, whose labels begin with "gre".

```{r}
x <- scan("/hdir/0/fmenares/Book/bookdown-master/data/mtdna.csv", what = character())
nchar(x)
## note the strings have different length
## because there is some header information
## and then because the labels are mixed with the sequences

## separate out the sequences
xx = x[nchar(x) > 10000] ## elements that contain both label and sequences
xx.list = strsplit(xx, split = ",") ## list of elements, split into label and sequences

## now split up this list into a vector of labels and a vector of sequences
get.first.element = function(x) {x[1]}
get.second.element = function(x) {x[2]}
labels = unlist(lapply(xx.list, get.first.element))
seqs = unlist(lapply(xx.list, get.second.element))

## Now use labels to select out the 20 Greeks
s <- grepl("^gre", labels)
my.labels = labels[s]

my.seqs = seqs[s]
nchar(my.seqs) ## note the sequences all have same number of bases.

## now put the bases in a matrix, with each column an indiviual and
## each row a base.
my.list <- strsplit(my.seqs, "")
A <- do.call(cbind, my.list)
dim(A)

## coding region sequences, 576-16023 (according to "Tree construction
## and haplogroup prediction" section, but not clear if this was used for
## Intrapopulation diversity
B <- A[576:16023,]
haps <- seqs
```

Let's inspect just a bit of one of these sequences
```{r}
print(nchar(haps[213]))
a_segment = substr(haps[213], 1, 100)
print(a_segment)
## tip: don't try to print the 16,000 character whole string. it will clog up your computer.
```


Let's put the haps in a matrix
```{r}
my.list <- strsplit(haps, "")
H <- do.call(cbind, my.list)
print(H[1:10, 1:4]) ## all the same
```

Let's find a site where there's polymorphism
```{r}
hap1 = H[,1]
hap2 = H[,2]
s <- min(which(hap1 != hap2))
head(H[s + -2:2, 1:4]) ## the polymorphic site in context
```

Let's see if that's hap1 is the only "A"
```{r}
table(H[s,])
```

We see that there are 19 individuals with this "A" instead of "G".

> Q: How many pairwise differences in total are there between hap1 and
> hap2?

These are the kind of comparisons _we_ will be doing.

### Ancient population estimates

Let's look at Figure 2 on page 5. 

> Q. What was effective population size in Ireland 1 thousand years ago
> (according to mtDNA)?

> Q. What was effective population size in Ireland 50 thousand years ago
> (according to mtDNA)?


> Q. For the "Irish", what is the annual population growth rate? What's the NRR?

The order of magnitude for each these populations appears to be about 10^4
in  last few KYA and 10^3 50 KYA. Together, perhaps the size is 10
fold. So we're looking at European effective population sizes on the
order of 100,000 the last few thousand years and on the order of
10,000 tens of thousands of years ago.


#### Using the Coalescent to estimate changing population size

Our procedure will involve a four steps:

1. Pick 100 _pairs_ of people at random and count their pairwise differences.

```{r}

set.seed(1)
hap_ids = 1:ncol(H)
hap_id_sample = sample(hap_ids,
                       size = 200,
                       replace = FALSE)

hap_id.mat <- matrix(hap_id_sample, 100, 2)

pairwise_diff_fun <- function(hap1, hap2)
{
    h1 <- hap1
    h2 <- hap2
##    h1 <- unlist(strsplit(hap1, ""))
##    h2 <- unlist(strsplit(hap2, ""))
    h1[h1 == "N"] <- NA ## note "N" means missing
    h2[h2 == "N"] <- NA ## making these NA avoids counting as polymorphism
    k = sum(h1 != h2, na.rm = T)
    n_valid = sum(!is.na(h1) & !is.na(h2))
    return(list(k = k, n_valid = n_valid))
}

pairwise_diff_fun(H[,1], H[,2])
```

Now we're ready to do pairwise comparisons of all 100 pairs of
haplotypes.

We'll define the fraction of locii that have mutated (the pairwise differences) as
$$
\bar{Y} = P/C
$$ 

```{r}

P.vec = NULL
C.vec = NULL
for (i in 1:nrow(hap_id.mat))
{
    hap_id.1 = hap_id.mat[i,1]
    hap_id.2 = hap_id.mat[i,2]
    hap1 = H[,hap_id.1]
    hap2 = H[,hap_id.2]
    out = pairwise_diff_fun(hap1, hap2)
    P.vec[i] = out$k
    C.vec[i] = out$n_valid
}

Y.bar = P.vec/C.vec
head(P.vec)
head(C.vec)
head(Y.bar)
```

2. Estimate 100 different times of MRCA ($T$)  using assumed mutation rate.

And now we'll use the mutation rate $\theta_m$ given by Batini to
compute time back to MRCA.

First, some background:

Let $a$ index sites and write $Y_a = 1$ if the letters are different,
and $Y_a = 0 otherwise. For 1 site, the probability that it there has been a mutation, is 1
minus the chance that there has been no mutation. 

$$
     P( Y_a = 1 ) = 1 - e^{-T\theta}
$$
The mean $\bar{Y}$ across the segment is the proportion of $Y_a$ that
equal 1. So, in expectation for the fraction of sites that mutate is
the same as the probability that 1 site mutates (assuming 
independence of mutation probabilities by site). This allows us to write

$$
     E{\bar{Y}} = 1 - e^{-2T\theta},
$$
where we've added a "2" in order to account that either one of the pairwise branched could have had a mutation, so we our "exposure" is twice the time to MRCA.

Rearranging we get an estimate $\hat{T} of $T$ to be
$$
\hat{T} = {-\log (1 - \bar{Y} ) \over \theta}
$$


Now we're ready to estimate the MRCAs
```{r}
theta_m = 2.21 * 10^(-8) ## Batini page 6 (TMRCA estimation)
T.vec <- -(1/2) * (1/theta_m) * log(1 - Y.bar) ## TMRCAs in years ago
head(T.vec, n = 10)
```

Let's visualize these
```{r}
hist(T.vec)
```

We see a lot of coalescence about 50 KYA, which is as far back as
Batini's estimates go. This means the population was small back
then. 

3. Estimate $h(t)$, the time-varying hazard of coalescence 

We'll do this in two ways. First we'll compute the slope of the
logarithm of the survival curve, but we'll see that it is noisy and
needs to be smoothed. Second, we'll construct a "life table" of
coalescence with discrete periods of time.

Let's start with the more continuous version of estimating slopes.

```{r}
## Plot survival curve by order of T
St = (100:1)/100 ## or more generally (length(T.vec):1)/length(T.vec)
t = kya = sort(T.vec)/1000
plot(kya, St, type = "l",
     xlab = "Kilo years ago", ylab = "Fraction of pairs without common ancestor",
     main = "Estimated probability of not coalescing")
plot(kya, log(St), type = "l",
     xlab = "Kilo years ago", ylab = "Log fraction of pairs without common ancestor",
     main = "Etimatated probability of not coalescing, log scale")
     
```

> Q: What is happening to slope in first from 1,000 to 50,000 years
> ago? 

> Q: What does this imply about hazard of coalescence?

> Q: What does this imply about effective population size?


 - a) Estimating hazards from smoothed survival curve

First we smooth. 

```{r}
out = lowess(x = kya, y = St, f = 1/5)

St_smooth = out$y
kya_smooth = out$x
plot(kya, St, cex = .5)
lines(kya_smooth, St_smooth, type = 'l')
```

> Q. Is f=1/5 a decent fit? Try a different value.  

Hazards as minus the slope of log

```{r}
haz_hat = -diff(St_smooth)/diff(kya_smooth)
plot(kya_smooth[-1], haz_hat, type = 'l')
```

> Q. Does this plot tell us anything about uncertainty?


- b) Life table appraoch

Now let's estimate the hazards using a "life table", where again
"death" is coalescence and "survival" is still not having a common
ancestor.

```{r}
## we choose these time boundaries arbitrarily ... not sure if
## we'll be able to see the "expansion" after ice age ...
## x = c(0, 12,  20, 40, 65,  180 )*1000    # time interval boundaries
x = c(0,2, 5, 10,  20, 30, 40, 65, 180) * 1000 ## time interval boundaries
```

Define a function to count "exposure" by those pairs that
have MRCA in time intervals

```{r}
get_nax <- function(Ti, x)
{
    ## get person years lived in interval by those who die
    nax <- NULL
    for (i in 1:(length(x)-1))
    {
        s <- Ti >= x[i] & Ti < x[i+1]
        if (length(Ti[s]) != 0) {
          nax[i] = mean(Ti[s] - x[i])
        }
        nax[is.na(nax)] <- 0
    }
    return(nax)
}
```

Construct the life table

```{r}
n <- diff(x)
T.vec.by.cat <- cut(T.vec, x, include.lowest = T, right = F)
ndx = table(T.vec.by.cat)
lx = rev(cumsum(rev(ndx)))
lxpn = c(lx[-1], 0)
nax = get_nax(Ti = T.vec, x = x)
nLx = n*lxpn + nax * ndx ## exposure
nmx = ndx/nLx ## hazard
lt <- cbind(x = x[-length(x)], n, ndx, lx, nax, nLx, nmx)
print(lt)
```

Let's compare the two estimates

```{r}
x.mid = x[-length(x)] + n/2
plot(x.mid, nmx, type = 'o')
axis(2)
lines(kya_smooth[-1] * 1000, haz_hat/1000, type = "l")
```

We're getting basically the same thing, with a little more hint of
rising hazards (shrinking pop size) in first 20 kya. Nearly the same
thing estimate 40 kya ago, and very little signal before that. 


4. Estimate the population size  $N_e(t)$

In order to estimate the population size, we have to think for a minute
about units.

Our hazards are per year, but our logic for why hazards are related to
population size is per generation. Remember, the chance of coalescence
_per generation_ was $1/N$. This means we will want to multiply the annual 
hazard by generation length in order to get hazards per generation unit of time.

Second, mtDNA is inherited only through mothers. So the coalescent
that we are thinking of is only for women. And the effective
population size we're estimating is for females only. We can get a rough estimate of 
both sexes by doubling the number of females.

Putting these two considerations together, we have
$$
\hat{N_e(both sexes)} = 2 \times {1 \over 25 \cdot h(t)},
$$
where the 2 inflates to both sexes, and the 25 inflates the annual
hazard into geneations of 25 years in length.

```{r}

Ne_smooth = 2 / (haz_hat/1000 * 25)

Ne_lifetable = 2 / (nmx * 25)

## create step function for plotting
Ne_lifetable_step = rep(Ne_lifetable, n/1000)
kya_step = 1:(max(x)/1000)

```

Plotting the results

```{r}
plot(kya_smooth[-1], Ne_smooth, type = 'l', ylim = c(1000, 60000), log = 'y',
     lty = 2)
lines(kya_step, Ne_lifetable_step, type = 'l', lwd = 2)
```

Uncertainty

> Q. How could we evaluate undercertainty? (Hint: resampling)

> Q. What does this estimate leave out?

(We probably won't do this in class. If we don't give it a try at
home.)



Let's loop through and do the whole estimation 40 times

```{r}
n_trials = 40
Ne.mat <- matrix(NA, nrow = n_trials, ncol = max(x)/1000)
set.seed(1)
for (r in 1:n_trials)
{
  #r = 1
    print(r)
    ## sample
    hap_id_sample = sample(hap_ids,
                           size = 200,
                           replace = FALSE)
    hap_id.mat <- matrix(hap_id_sample, 100, 2)
    ## estimate MRCA distribution
    P.vec = NULL
    C.vec = NULL
    for (i in 1:nrow(hap_id.mat))
    {
        hap_id.1 = hap_id.mat[i,1]
        hap_id.2 = hap_id.mat[i,2]
        hap1 = H[,hap_id.1]
        hap2 = H[,hap_id.2]
        out = pairwise_diff_fun(hap1, hap2)
        P.vec[i] = out$k
        C.vec[i] = out$n_valid
    }
    Y.bar = P.vec/C.vec
    T.vec <- -(1/2) * (1/theta_m) * log(1 - Y.bar) ## TMRCAs in years ago
    ## estimate Ne
    T.vec.by.cat <- cut(T.vec, x, include.lowest = T, right = F)
    ndx = table(T.vec.by.cat)
    lx = rev(cumsum(rev(ndx)))
    lxpn = c(lx[-1], 0)
    nax = get_nax(Ti = T.vec, x = x)
    nLx = n*lxpn + nax * ndx ## exposure
    nmx = ndx/nLx ## hazard
    Ne_lifetable = 2 / (nmx * 25)
    ## create step function for plotting
    Ne_lifetable_step = rep(Ne_lifetable, n/1000)
    kya_step = 1:(max(x)/1000)
    ## save result
    Ne.mat[r,] <- Ne_lifetable_step
}

Ne.interval <- apply(Ne.mat, 2, quantile, c(.1,.5, .9))
matplot(t(Ne.interval), type = 'l', log = 'y', col = "grey", lty = 1, lwd = 2)
lines(Ne.interval["50%",], lwd = 4)
```

So it seems fairly clear that effective population size has been
growing the last 50 thousand years, from a low of a few thousand to a
few tens of thousand. 

The general trend in growth is consistent with Batini but

* Our total population size seems smaller by a factor of about 4 or 5.

* We don't have the resolution to see increase between 10 and 20 kya,
  the end of the Last Glacial Maximum or the more recent Bronze Age
  steppe expansion 2 to 5 kya.
  
In order to get more resolution and study sub-group differences, we
would want to turn to methods that do more than pair-wise comparisons,
giving us more detailed information about the effective population
sizes of the past.


## Some exercises

1. Try doing 150 pairwise comparisons (instead of 100) and see what
   happens.

2. Try doing 50 pairwise comparisons (instead of 100) and see what
   happens.
   
3. Try changing the interval sizes (or the smoothing parameter) to see
   if you can see either the post-glaciation population increase or
   the Bronze Age increase? (I don't know if it's possible with our
   methods.)
   
4. Figure out how to use BEAST or some other software and
   reproduce their results for one sub-population and tell us what you learned.
   
   

   











<!--chapter:end:09-coalescent.Rmd-->

# Student Presentations
## Mortality Crossovers

### Outline
- Paper presentations
  - @coale1986mortality
  - @manton1981methods
- Empirical examples of crossovers with CenSoc data
- Investigate quality of age of death reporting in CenSoc

### _Mortality Crossovers: Reality or Bad Data?_ [@coale1986mortality]
They noticed the following patterns in the age-specific death rates for different countries. 

```{r crossover, fig.cap='Age specific death rates for different countries and cohorts. Source: @coale1986mortality', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_1.png')
    ```   
    

- Selection / heterogeneity:
  - Elimination of the frailer members of the population at younger ages leaves only the very robust with lower mortality rates.
- Level playing fields at older ages
  - Social Security, Medicare, etc.
- Bad data
  - Misreporting age of death can lead to biased estimates of mortality rates at older ages


#### Age heaping:
- General pattern of age misstatement, most often rounding up to nearest 5 or 10.
- Begins with a modest upward transfer at age 60 or 70, increases rapidly with age.

```{r crossover2, fig.cap='Age heaping example Source: IPUMS International', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_2.png')
    ```  

  Implications of age heaping:
  
  $\text{age heaping on age 70} = \frac{Pop_{70}}{(Pop_{69}+Pop_{71})/2}$
  
```{r crossover3, fig.cap='Age heaping. Source: @coale1986mortality', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_3.png')
    ```

#### Takeaways
- Age overstatement at advanced ages is common and downwardly biases estimates of mortality rates
- Age heaping is associated with age overstatement
- Low quality mortality data can artificially create a mortality crossover


### _Methods for Evaluating the Heterogeneity of Aging Processes in Human Populations Using Vital Statistics Data: Explaining the Black/White Mortality Crossover by a Model of Mortality Selection_ [@manton1981methods]

#### Summary
- A model to compute the ratio of Black and White individual age specific mortality risks (within sex) to determine if the adjustments of heterogeneity and mortality selection is sufficient to remove the crossover.
- Data from the U.S. Black and White populations for the period 1935 to 1975.
- Mortality crossover (Blacks having relatively lower mortality rates) at age 75.
- Could be explained under the proposed model.
- Data quality? Variety of evidence supporting the existence of a crossover.
- Consequently, careful consideration should be made of the population
mechanisms by which the crossover might occur.

#### A model of selection
- Life tables are separately calculated for the Black and White populations in the U.S. over the period 1935 to 1975 based upon the assumptions:
  - Each population is heterogeneous.
  - The initial distribution of individuals in each population is identical (within sex) with respect to variables relevant to longevity.
  -Individual's environmental conditions are fixed at birth.
- Operationally, they modified standard life table calculations (Chiang, 1968) to reflect the dependence of mortality rates at advanced ages upon the selection of earlier mortality levels on a heterogeneous population.

#### A little bit of math
- Assumptions:
  - The following partial differential equation governs the change of the distribution as cohort age:
  $$\begin{aligned}
  \frac{\partial f_{x}(z)}{\partial x}= f_{x}(z)(\bar{\mu}_{x}-\mu_{x}(z))
  \end{aligned}$$
  - Each person retains the value of $z$ (longevity characteristics) given at birth.
  - Functional forms: 
  $$\begin{aligned}
  \mu_{x}(z)=z\mu_{x}(1)= z\mu_{x}
  \end{aligned}$$
  - Thus $z$ may be taken to be a measure of relative (to the standard individual) frailty or "susceptibility to death". Alternatively, $1/z$ may be considered as a measure of vitality or "robustness".
- Variance and frailty relation:
  - We also have the following definitions:
  $$\begin{aligned}
  \overline{\mu}_x&= \overline{z}\mu_x\\
  \overline{z}_x&= \int_0^\infty z f_x(z) dz
  \end{aligned}$$
  - Therefore, we can say that $\frac{\partial\overline{z}_x}{\partial x}=- \mu_x \sigma_x^2(z)$
  $$\begin{aligned}
  \frac{\partial f_x(z)}{\partial x}&=f_x(z)(\overline{z}\mu_x - z\mu_x)\\
  \frac{\partial\overline{z}_x}{\partial x}&= \frac{\partial \int z f_x(z) dz}{\partial x}\\
  &= \int z \frac{\partial f_x(z) }{\partial x}dz\\
  &= \int z f_x(z)(\overline{z}\mu_x - z\mu_x)dz \\
  &= \mu_x \left(\int z f_x(z)\overline{z} dz-  \int z^2 f_x(z)dz\right)\\
  &= -\mu_x \left(\int z^2 f_x(z)dz - \overline{z} \int z f_x(z) dz  \right)\\
  &= -\mu_x \left(\int z^2 f_x(z)dz - \overline{z}^2  \right)\\
  \frac{\partial\overline{z}_x}{\partial x}&=  - \mu_x \sigma_x^2(z)
  \end{aligned}$$
  - This means the "frailer" population members (with high z's) are being selected earlier than their more "robust" contemporaries (with low z's)
- Gamma distribution:
  - The proportionality assumption has implications for $f_x(z)$
      - Mortality cannot be negative, then z must be positive.
      - Average endowment, $\overline{z}_0$, for longevity, it follows that $\overline{\mu}_0=\mu_0$. Hence, $\overline{z}_0=1$
      - It would be desirable that the paramteres of $f_x$ be unchanged for any $x$.
  $$\begin{aligned}
  f_x(z)=z^{k-1}\lambda_x^k exp(-z\lambda_x)/\Gamma(K)
  \end{aligned}$$
        - With mean $\overline{z}_x=k/\lambda_x$ and variance:
  $$\begin{aligned}
  \sigma_x^2(z) = \overline{z}^2_x/k = k/\lambda^2
  \end{aligned}$$
- ASPD and the relative risk:
  - From the variance of the gamma, the average mortality and the definition of $\overline{s}_x$, we have:
  $$\begin{aligned}
\sigma_x^2(z)&= \overline{z}^2_x/k\\
\overline{\mu}_x&= \overline{z}_x\mu_x\\
\overline{s}_x&= exp\left(-\int_0^x \overline{\mu}_t dt\right)\\
nq_x(z) &= 1-exp\left(\frac{kz}{\overline{s}_x^{1/k}}-\frac{kz}{\overline{s}_{x+n}^{1/k}}\right)\\
\overline{r}_x&= \frac{\mu_{x1}}{\mu_{x2}}\left(\frac{\overline{s}_{x1}}{\overline{s}_{x2}}\right)^{1/k}
  \end{aligned}$$

#### Parameter k
- Select values of k focus upon the biological rather than statistical
  a. Biological dimensions underlying longevity are normally distributed at birth.
  b. Any deviation from an "optimal" biological point will be associated with decreased survival.
  c. Conditionally on age, mortality will be a quadratic function
  d. Each individual's endowment for longevity (z) is fixed at birth.
- The value of к is the result of n, number of dimensions relevant to longevity.
- The relation of n to the gamma shape parameter is simply n=2k.
- Lower n, the greater is the heterogeneity (higher variance of gamma)
- The values of n used are 1 and 2, suggesting that longevity is unidimensional (k=0.5) and bidimensional (k=1), respectively.

```{r crossover4, fig.cap='Cohort and Individual Age Specific Mortality Probabilities for the 1875 White Female Birth Cohort. Source: @manton1981methods', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_4.png')
```  
```{r crossover5, fig.cap='Age-specific mortality risk ratios (Black males vs White males) for the years 1935,1955 and 1975. Source: @manton1981methods', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_5.png')
```  
```{r crossover6, fig.cap='Age-specific mortality risk ratios (Black females vs White females) for the years 1935,1955 and 1975. Source: @manton1981methods', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_6.png')
```  

#### Takeaways

  - It seems that the crossover at advanced ages for males is an artifact of the early differential mortality selection.
  - An explanation for this differentials at older ages is the relatively more rapid reduction in individual white male mortality
  - For black females we can see that they were worse off than males between 25-45 (the childbearing years) at 1935.
  - Different Zs will result in a divergence between the increase with age of the cohort mortality rates and the age increase in the probabilities of death for individuals within the cohort.
  - This because the earlier selection of the less "robust" population members, implies that individuals age "faster" than their cohorts.
  
### CenSoc Mortality Crossovers
```{r crossover7, fig.cap='Pooled cohorts of 1890-1900', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_7.png')
```  

```{r crossover8, fig.cap='Pooled cohorts of 1890-1900, by education level', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_8.png')
```  
  
```{r crossover9, fig.cap='Pooled cohorts of 1890-1900 by location', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_9.png')
```  

```{r crossover10, fig.cap='Pooled cohorts of 1890-1900 by wages', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_10.png')
```  
#### Can the crossover be eliminated?

The baseline hazard is given by  $\mu_{0}(x)=\bar{\mu}(x)e^{\sigma^{2}\bar{H}(x)}$ and we try different values of $\sigma^2$ for each group of people, in this case Black and White. 

```{r crossover11, fig.cap='Pooled cohorts of 1890-1900 by wages', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_11.png')
```  

#### Is death data in CenSoc file less reliable for Blacks?

  - 57% of black people in pooled 1890-1900 cohort are missing death
days (day of the month)
  - 51% of white people missing death days
  - No missing death months
  - Missing day of birth far less common (about 0.005% of records)
  - Are missing dates indicative of poor data?

```{r crossover12, fig.cap='Day of death', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_12.png')
```  

```{r crossover13, fig.cap='Month of death', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_13.png')
```  

If observations with missing dates are dropped then the mortality rates look slightly different.

```{r crossover14, fig.cap='Mortality crossover for pooled cohorts of 1890-1900.', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_14.png')
```  

#### Heaping for reported birth year

  - No Heaping on Death Year
  - No Heaping on Age of Death
  - Heaping on Birth Year
  
$Heaping(1900)=\frac{B_{1900}}{(B_{1899}+B_{1901})/2}$

```{r crossover15, fig.cap='Heaping: birth year', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/crossover_15.png')
```  


## Plateaus
- @steinsaltz2006understanding
- @barbi2018: Recent paper claiming to be “the best evidence to date for the existence of extreme-age mortality plateaus in humans”

Using total mortality rates for italian cohorts from 1900 to 1926 (from HMD), we can look at mortality rates over ages and cohorts, as an introduction to @barbi2018. When focusing on mortality rates above age 60, we see a constantly increasing curve with some variation at very old ages (beyond 100). One important limitation of our life tables is that the rates are only observed up until age 109.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)

it <- read_table("data/ITA.bltcoh_1x1.txt", skip=1)
it$Age <- as.numeric(it$Age)
it$mx <- as.numeric(it$mx)
it$qx <- as.numeric(it$qx)
it <- it %>% mutate(mx = if_else(mx == 0, as.numeric(NA), mx))

```
```{r plateau1, fig.cap='Log mortality rates above age 60 after 1900', out.width='80%', fig.align='center',  echo=FALSE}
ggplot(it %>% filter(Age > 60, Year > 1900)) + 
  geom_line(aes(x = Age, y = log(mx), color = as.factor(Year))) + 
  theme_minimal()+
 ylab(expression(log(m[x])))+
  labs(color= "Year")

```

To detect plateaus above age 105 like the paper, we estimate a Gompertz on ages 60- 100 (there's some noise from the war) after 1900. Recall that to estimate mortality using hazard rates for Gompertz: 
\begin{aligned*}
h(x) = a e^{b*x} \\
log(h{_x}) = log(a) + b\times x
\end{aligned*}
Since we don't have $h(x)$ from the lifetable, we use ${_1}m_x$

```{r, message=FALSE, warning=FALSE}
min_gomp_age <- 60
max_gomp_age <- 105
max_plateau_age <- 109
min_year <- 1900 

gomp <- lm(log(mx) ~ Age, data = it %>% filter(Age > min_gomp_age & Age < max_gomp_age & Year > min_year))
new <- data.frame(Age = max_gomp_age:max_plateau_age)
new$Gomp.pred <- exp(predict(gomp, new))
new <- new %>% left_join(it %>% filter(Age > max_gomp_age-1, Year > min_year) %>% select(Age, Year,mx) %>% spread(Year, mx))
new <- new %>% gather("Year", "mx", -Age)
```
It seems that the Gompertz prediction is in line with the data, at least in its trend. However, note the variation in rates between every cohort. 
```{r plateau2, fig.cap='Predicted log mortality rates above age 104 after 1900', out.width='80%', fig.align='center',  echo=FALSE}
ggplot(new %>% filter(Year != "Gomp.pred")) + 
  geom_line(aes(Age, log(mx), color = Year)) + 
  geom_line(data = (new %>% filter(Year == "Gomp.pred")), aes(Age, log(mx) ), color = "black", size = 2) + 
  ggtitle("Gompertz prediction (black) ")+
  theme_minimal()+
  ylab(expression(log(m[x])))

```
 Now, we look at predicted mortality rates by gender. The graphs below show a similar trend as the previous graph for overall mortality rates. Although the predictions seem to fit the data, that is Gompertz is an adequate method to model mortality between the ages of 100 to 109, a plateau may appear for even later ages. Since HMD data uses ages up to 109, we are unable to observe a plateau as in @barbi2018.

```{r, message=FALSE, warning=FALSE}
# For men:
it_m <- read_table("data/ITA.mltcoh_1x1.txt", skip=1)
it_m$Age <- as.numeric(it_m$Age)
it_m$mx <- as.numeric(it_m$mx)
it_m$qx <- as.numeric(it_m$qx)
it_m <- it_m %>% mutate(mx = if_else(mx == 0, as.numeric(NA), mx))


gomp <- lm(log(mx) ~ Age, data = it_m %>% filter(Age > 60 & Age < 100 & Year > 1900))
new <- data.frame(Age = 101:109)
new$Gomp.pred <- exp(predict(gomp, new))
new <- new %>% left_join(it_m %>% filter(Age > 95, Year > 1900) %>% select(Age, Year,mx) %>% spread(Year, mx))
new <- new %>% gather("Year", "mx", -Age)

```

```{r plateau3, fig.cap='Log mortality rates for men above age 60 after 1900', out.width='80%', fig.align='center',  echo=FALSE, warning=FALSE}
ggplot(it_m %>% filter(Age > 60 & Year > 1900)) + 
  geom_line(aes(x = Age, y = log(mx), color = as.factor(Year)))+
  theme_minimal()+
 ylab(expression(log(m[x])))+
  labs(color= "Year")
```

```{r plateau4, fig.cap='Predicted log mortality rates for men above age 99 after 1900', out.width='80%', fig.align='center',  echo=FALSE, warning=FALSE}
ggplot(new %>% filter(Year != "Gomp.pred")) + 
  geom_line(aes(Age, log(mx), color = Year)) + 
  geom_line(data = new %>% filter(Year == "Gomp.pred"), aes(Age, log(mx) ), color = "black", size = 2) +
  ggtitle("Male: Gompertz Prediction (black)")

```

```{r, message=FALSE, warning=FALSE}
# Women
it_f <- read_table("data/ITA.fltcoh_1x1.txt", skip=1)
it_f$Age <- as.numeric(it_f$Age)
it_f$mx <- as.numeric(it_f$mx)
it_f$qx <- as.numeric(it_f$qx)
it_f <- it_f %>% mutate(mx = if_else(mx == 0, as.numeric(NA), mx))

gomp <- lm(log(mx) ~ Age, data = it_f %>% filter(Age > 45 & Age < 100 & Year > 1900))
new <- data.frame(Age = 101:109)
new$Gomp.pred <- exp(predict(gomp, new))
new <- new %>% left_join(it_f %>% filter(Age > 95, Year > 1900) %>% select(Age, Year,mx) %>% spread(Year, mx))
new <- new %>% gather("Year", "mx", -Age)
```

```{r plateau5, fig.cap='Log mortality rates for women above age 60 after 1900', out.width='80%', fig.align='center',  echo=FALSE, warning=FALSE}
ggplot(it_f %>% filter(Age > 60 & Year > 1900)) + 
  geom_line(aes(x = Age, y = log(mx), color = as.factor(Year))) +
  theme_minimal()+
  ylab(expression(log(m[x])))+
  labs(color= "Year")
```

```{r plateau6, fig.cap='Predicted log mortality rates for women above age 99 after 1900', out.width='80%', fig.align='center',  echo=FALSE, warning=FALSE}
ggplot(new %>% filter(Year != "Gomp.pred")) + 
  geom_line(aes(Age, log(mx), color = Year)) + 
  geom_line(data = new %>% filter(Year == "Gomp.pred"), aes(Age, log(mx) ), color = "black", size = 2) + 
  theme_minimal()+
  ggtitle("Female: Gompertz Prediction (Black) ")
```



## Rising Inequality

- @waldron2007trends: Age-cohort model suggesting growing inequality by income, but includes many caveats about heterogeneity. Our challenge is to apply models of heterogeneity to this issue, particularly the result on mortality improvement over time.

### _Trends in mortality differentials and life expectancy for male social security-covered workers, by socioeconomic status_ [@waldron2007trends]
  
  - This paper fits in nicely with our discussion of heterogeneity and different rates of improvement across groups
  - Analyzes trends in mortality differentials and life expectancy by average relative earnings for male Social Security-covered workers aged 60 or older
  - Finds differences in level and rate of change in mortality improvement over time by SES
      - "..male Social-Security covered workers born in 1941 who had average relative earnings in the top half of the earnings distribution and who lived to age 60 would be expected to live 5.8 more years than their counterparts in the bottom half. In contrast, among male Social Security-covered workers born in 1912 who survived to age 60, those in the top half of the earnings distribution would be expected to live only 1.2 years more than those in the bottom half.”
  - Warns that these projections are very much only one possible outcome, since the causes of the widening differentials observed are still not understood

#### Context and data

  - Historically, mortality inequalities by class that emerged between 1650-1850 began to narrow by the 1930s and 1940s (eg. Antonovsky 1967)
  - Evidence that the gap has widened due to differential rates of decline in deaths due to heart disease (Feldman 1989)
  - This paper adds to the literature by using a large longitudinal data set in which deaths are observed over 29 years
  - This allows for disaggregation by age and year-of-birth, avoiding linearity assumptions with regard to interaction terms
  - Wage data comes from the SSA Continuous Work History Sample, combined with the Numident (master death) file and the Master Beneficiary Record file
  - Earnings are measured relative to the national average wage in the year, then averaged
  - Earnings are only used post 1957 to account for expansion of Social Security coverage--some issues still exist due to subsequent expansions, but Waldron suggests that these should not have an important
  - Important caveat: this is not a representative sample, as it excludes men not participating in the labor force and does not account for the possibility of low covered earnings combined with high non-covered earnings

#### Methods

  - Estimates are constructed of mortality differentials and cohort and period life expectancies
  - Models:
    - Mortality differentials over time: 
      - $1(dead)_{isc}= \alpha +\beta_{1}age_{isc} + \beta_{2}1(earnings)_{isc}+\epsilon_{isc}$
    - Cohort life expectancy estimates:
      - Uses a discrete-time logistic regression model of the form
      - $1(dead)_{isc}= \gamma + \theta_{1}age_{isc}+ \theta_{2}birthyear_{isc} + \theta_{3}age_{isc}\times birthyear_{isc}+ \theta_{4}1(earnings)_{isc}+ \theta_{5}age_{isc}\times 1(earnings)_{isc}+ \theta_{6}birthyear_{isc}\times 1(earnings)_{isc}+ \theta_{7}age_{isc}\times birthyear_{isc}\times 1(earnings)_{isc}+\varepsilon_{isc}$
    - Does not control for changes in sample frailty over time: “Theoretically, if more frail members of lower-earnings groups are making it into the sample at older ages than in the past, then they could push up mortality differentials relative to the past. Hypothetically, it is possible that widening mortality differentials can indicate improvement for the lower-earnings groups, if such widening is an indication of their survival in greater numbers to ages at which previously only the strongest among them survived.”


#### Results

  - Widening mortality differentials: 
```{r progress1, fig.cap='Selected cohort survival curves for male Social Security-covered workers, by age and earnings group. Source: Chart 1 @waldron2007trends', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/progress_1.png')
```  
```{r progress2, fig.cap='Percentage change in the death rate for male Social Security-covered workers, by selected age and earnings group from birth years 1912-1941. Source: Chart 2 @waldron2007trends', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/progress_3.png')
```  
```{r progress3, fig.cap='Cohort life expectancy at age 65 (and 95 percent confidence intervals) for male Social Security-covered workers, by selected birth years and earnings group. Source: Chart 3 @waldron2007trends', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/progress_3.png')
```  
  - These results also hold for cohort life expectancies, though for later birth cohorts the confidence intervals tend to overlap--projected estimates.
  - Waldron, using period data, also attempts to compare these numbers to other OECD countries, finding that, at 65, high-earning Social Security-covered men rank close to population averages for multiple other countries like Canada, suggesting they do worse than high-earning men in these countries
    - Men in the bottom quarter of the earning distribution “could expect to live roughly as long as the average Irishman”
    - Differences in medical care and health behaviors?
    - Short discussion of the differences in frailty across countries--Mexico is predicted to have a selectively healthier and more robust population at age 80
  - This paper offers insight into the way in which population heterogeneity may lead to very different outcomes--though questions about sample frailty remain.
  - A simulation of mortality differentials over time shows that:
    - Over time, for a given age, the differential has widened.
    - Within the same cohort, across ages, the differential falls.
  ```{r progress4, fig.cap='Odds ratio (confidence intervals) for the bottom half of the earnings distribution relative to the top half of the distribution, by year of birth and age. Source: Table 1 @waldron2007trends', out.width='80%', fig.align='center',  echo=FALSE}
    knitr::include_graphics('figures/progress_4.png')
```   
  - We can replicate the results of the simulation below. We want to see if heterogeneity alone can explain the pattern in
Waldron's observations of hazard ratios declining by age and increasing from one cohort to the next. The key idea is that moving from one cohort to the next, holding age constant, is like moving to younger ages within a cohort. Just as moving to younger ages makes the observed hazard ratio larger, moving to a lower baseline mortality rate, holding age constant,
also makes the hazard ratio larger. 
```{r}
 ## Waldron Simulation using Gamma-Gompertz
t <- 1:5 ## five different cohrots (say each 10 years apart)
k <- .25 ## amount of mortality decline per decade (about 2.5% per year)
b <- .1 ## gompertz slope
a0 <- 10^{-3} ## starting hazard (rather high)
a0.vec <- a0 * exp(-k * t) #hazard vector for every cohort, including a mortality decline
x <- 0:100

# mu.bar <- a * exp(b * x) / (1 + (a * s2 / b) * (exp(b*x) - 1)) #gamma gompertz formula, where s2 is sigma^2
mu.1.bar.xt <- matrix(NA, length(t), length(x)) # matrix of cohorts (rows) by ages (columns)
mu.2.bar.xt <- matrix(NA, length(t), length(x))
dimnames(mu.1.bar.xt) <- list(t, x)
dimnames(mu.2.bar.xt) <- list(t, x)
s2 <- .2

for (i in 1:length(t)) #i: cohort counter
{
    a1 <- a0.vec[i]
    a2 <- a1 * 2                           
    mu.1.bar.xt[i,] <- a1 * exp(b * x) /
        (1 + (a1 * s2 / b) * (exp(b*x) - 1)) # baseline hazard
    mu.2.bar.xt[i,] <- a2 * exp(b * x) /
        (1 + (a2 * s2 / b) * (exp(b*x) - 1)) # population hazard
}

R.mat <- mu.2.bar.xt/mu.1.bar.xt #odds ratio

waldron.simu <- R.mat[, x %in% seq(60, 100, 10)]
print(waldron.simu)
```
  - Interestingly, you can see that after 5 decades, the $R(x)$ curve has shifted over almost exactly 10 years. R(90, decade 1) nearly equals R(100, decade 5) and similarly for age 60, 70, and 80.
  - We now block the lower right cells (so that our table looks like Waldron's):
    ```{r, echo=FALSE}
waldron.upper <- waldron.simu
waldron.upper[2,5] <- NA
waldron.upper[3,4:5] <- NA
waldron.upper[4,3:5] <- NA
waldron.upper[5,2:5] <- NA

print(round(waldron.upper,2))
```
  - We see a more rapid decline by age, and a less rapid increase by cohort. But qualitatively the pattern looks similar.
  - Further investigation could try to choose more accurate $a0$ and $b$ parameters to match better with observed mortality rates. Note: the true risk of being in group 2 instead of 1 is R = 2. So it's not that by cohort 5 we're seeing a distortion of the R(60). Rather we're seeing convergence of the population value to the individual risk.



<!--chapter:end:student_presentations.Rmd-->

